"""
Academic Sorting Analyzer - An√°lisis y Comparaci√≥n de Algoritmos de Ordenamiento
==================================================================================

Este m√≥dulo proporciona un framework completo para analizar y comparar el rendimiento
de 12 algoritmos de ordenamiento diferentes aplicados a datos acad√©micos (art√≠culos,
papers, publicaciones) extra√≠dos de bases de datos como EBSCO.

Algoritmos Implementados:
-------------------------
1. TimSort - O(n log n) - Algoritmo h√≠brido usado por Python
2. CombSort - O(n log n) promedio - Mejora de BubbleSort
3. SelectionSort - O(n¬≤) - Algoritmo b√°sico de selecci√≥n
4. TreeSort - O(n log n) promedio - Basado en √°rbol binario
5. PigeonholeSort - O(n + k) - Distribuci√≥n por categor√≠as
6. BucketSort - O(n + k) promedio - Distribuci√≥n en buckets
7. QuickSort - O(n log n) promedio - Divide y conquista
8. HeapSort - O(n log n) - Basado en heap binario
9. BitonicSort - O(n log¬≤n) - Para procesamiento paralelo
10. GnomeSort - O(n¬≤) promedio - Similar a InsertionSort
11. BinaryInsertionSort - O(n¬≤) - InsertionSort optimizado
12. RadixSort - O(d*(n+k)) - Ordenamiento por d√≠gitos

Funcionalidades:
----------------
- Ejecuci√≥n y medici√≥n de tiempo de todos los algoritmos
- Generaci√≥n de gr√°ficos comparativos
- An√°lisis de autores m√°s frecuentes
- Exportaci√≥n de resultados ordenados
- Reportes detallados de rendimiento

Criterio de Ordenamiento:
--------------------------
Los datos se ordenan por:
1. A√±o de publicaci√≥n (ascendente)
2. T√≠tulo del art√≠culo (alfab√©tico, case-insensitive)

Uso T√≠pico:
-----------
>>> analyzer = AcademicSortingAnalyzer("articles.csv")
>>> results = analyzer.run_all_algorithms()
>>> analyzer.create_time_comparison_chart(results)
>>> top_authors = analyzer.get_top_authors(15)

O usar la funci√≥n de conveniencia:
>>> analyze_academic_data("articles.csv", "output_analysis")

Autor: [Tu nombre]
Fecha: 2025
Licencia: [Tu licencia]
"""

import pandas as pd
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import seaborn as sns
import time
import re
from typing import List, Tuple, Dict
from collections import Counter
import numpy as np


class TreeNode:
    """
    Nodo para implementaci√≥n de Tree Sort (√°rbol binario de b√∫squeda).
    
    Esta clase representa un nodo individual en un √°rbol binario de b√∫squeda
    utilizado por el algoritmo TreeSort. Cada nodo almacena un valor y
    referencias a sus hijos izquierdo y derecho.
    
    Attributes:
        val: Valor almacenado en el nodo. Puede ser cualquier tipo comparable.
        left (TreeNode): Referencia al hijo izquierdo (valores menores).
        right (TreeNode): Referencia al hijo derecho (valores mayores o iguales).
    
    Example:
        >>> node = TreeNode((2023, "Machine Learning", 0))
        >>> node.val
        (2023, 'Machine Learning', 0)
        >>> node.left is None
        True
        >>> node.right is None
        True
    """
    def __init__(self, val):
        """
        Inicializa un nodo del √°rbol binario.
        
        Args:
            val: Valor a almacenar en el nodo. T√≠picamente una tupla
                (a√±o, t√≠tulo, √≠ndice) para ordenamiento de datos acad√©micos.
        """
        self.val = val
        self.left = None
        self.right = None


class AcademicSortingAnalyzer:
    """
    Analizador completo de algoritmos de ordenamiento para datos acad√©micos.
    
    Esta clase implementa 12 algoritmos de ordenamiento diferentes y proporciona
    herramientas para comparar su rendimiento, visualizar resultados y analizar
    datos acad√©micos (art√≠culos, publicaciones, papers).
    
    La clase carga datos desde un CSV, los prepara para ordenamiento (extrayendo
    a√±os de fechas, normalizando t√≠tulos), ejecuta m√∫ltiples algoritmos midiendo
    su tiempo de ejecuci√≥n, y genera reportes y visualizaciones comparativas.
    
    Attributes:
        csv_file (str): Ruta al archivo CSV con datos acad√©micos
        df (pd.DataFrame): DataFrame con los datos cargados y preparados
    
    Expected CSV Structure:
        El CSV debe contener al menos estas columnas:
        - title: T√≠tulo del art√≠culo
        - publication_date: Fecha de publicaci√≥n (cualquier formato)
        - authors: Autores separados por punto y coma (opcional)
        
    Prepared Data Columns:
        Despu√©s de load_data(), se agregan:
        - title_clean: T√≠tulo limpio (sin NaN)
        - year: A√±o extra√≠do de publication_date
        - sort_key: Tupla (year, title_normalized) para ordenamiento
    
    Example:
        >>> # Uso b√°sico
        >>> analyzer = AcademicSortingAnalyzer("ebsco_articles.csv")
        >>> results = analyzer.run_all_algorithms()
        >>> analyzer.create_time_comparison_chart(results)
        
        >>> # An√°lisis de autores
        >>> top_authors = analyzer.get_top_authors(15)
        >>> print(top_authors.head())
        
        >>> # Reporte completo
        >>> analyzer.generate_complete_report("analysis_2025")
    
    Performance Notes:
        - Para datasets peque√±os (<1000 registros), todos los algoritmos son r√°pidos
        - Para datasets grandes (>10000 registros), usar TimSort, QuickSort, HeapSort
        - Algoritmos O(n¬≤) como SelectionSort pueden ser muy lentos con >5000 registros
    """
    
    def __init__(self, csv_file: str):
        """
        Inicializa el analizador con un archivo CSV de datos acad√©micos.
        
        Args:
            csv_file (str): Ruta al archivo CSV que contiene los datos acad√©micos.
                El archivo debe existir y ser un CSV v√°lido con encoding UTF-8.
                Debe contener al menos las columnas 'title' y 'publication_date'.
        
        Side Effects:
            - Llama autom√°ticamente a load_data() para cargar el CSV
            - Imprime mensajes de carga y preparaci√≥n en consola
        
        Raises:
            Exception: Si hay error al cargar el archivo (propagado desde load_data)
        
        Example:
            >>> analyzer = AcademicSortingAnalyzer("articles.csv")
            ‚úÖ Datos cargados: 1,234 registros
            üìã Columnas disponibles: ['title', 'authors', 'publication_date', ...]
            üìä Datos preparados con 1,234 registros v√°lidos
        """
        self.csv_file = csv_file
        self.df = None
        self.load_data()
        
    def load_data(self):
        """
        Carga los datos desde el archivo CSV y los prepara para ordenamiento.
        
        Lee el CSV especificado en __init__, muestra informaci√≥n sobre las
        columnas disponibles y llama a _prepare_data() para limpiar y
        normalizar los datos.
        
        Side Effects:
            - Popula self.df con el DataFrame cargado
            - Llama a _prepare_data() para agregar columnas calculadas
            - Imprime mensajes informativos en consola
        
        Raises:
            Exception: Si el archivo no existe, no es CSV v√°lido, tiene
                encoding incorrecto, o cualquier otro error de lectura.
                En caso de error, inicializa self.df como DataFrame vac√≠o
                para evitar AttributeError posteriores.
        
        Example:
            >>> analyzer = AcademicSortingAnalyzer("articles.csv")
            # load_data() se llama autom√°ticamente
            ‚úÖ Datos cargados: 5,432 registros
            üìã Columnas disponibles: ['id', 'title', 'authors', 'publication_date', ...]
            üìä Datos preparados con 5,432 registros v√°lidos
        
        Note:
            Si el CSV no tiene encoding UTF-8, puede fallar. Considera
            modificar el par√°metro encoding seg√∫n tus necesidades.
        """
        try:
            # Cargar CSV con pandas
            self.df = pd.read_csv(self.csv_file, encoding='utf-8')
            print(f"‚úÖ Datos cargados: {len(self.df)} registros")
            
            # Mostrar columnas disponibles para informaci√≥n del usuario
            print(f"üìã Columnas disponibles: {list(self.df.columns)}")
            
            # Limpiar y preparar datos para ordenamiento
            self._prepare_data()
            
        except Exception as e:
            print(f"‚ùå Error cargando datos: {e}")
            # Asegurar que self.df no quede en None para evitar AttributeError posteriores
            self.df = pd.DataFrame([])
            raise

    def _prepare_data(self):
        """
        Prepara los datos para ordenamiento a√±adiendo columnas calculadas.
        
        M√©todo privado que realiza las siguientes transformaciones:
        1. Crea 'title_clean': T√≠tulo sin valores NaN, convertido a string
        2. Crea 'year': A√±o extra√≠do de 'publication_date' (0 si no hay fecha)
        3. Crea 'sort_key': Tupla (year, title_normalized) para ordenamiento
        
        La columna 'sort_key' es cr√≠tica ya que define el criterio de ordenamiento
        usado por todos los algoritmos: primero por a√±o ascendente, luego por
        t√≠tulo alfab√©tico (case-insensitive).
        
        Side Effects:
            - A√±ade columnas 'title_clean', 'year', 'sort_key' a self.df
            - Imprime mensaje de confirmaci√≥n con n√∫mero de registros
        
        Example:
            >>> # Llamado autom√°ticamente por load_data()
            >>> # Despu√©s de preparar datos:
            >>> print(analyzer.df[['title', 'title_clean', 'year', 'sort_key']].head())
            
        Note:
            Si el DataFrame est√° vac√≠o o es None, no hace nada y solo
            imprime una advertencia.
        """
        if self.df is None or self.df.empty:
            print("‚ö†Ô∏è DataFrame vac√≠o. No se preparan datos de ordenamiento.")
            return
            
        # ===== PASO 1: LIMPIAR T√çTULO =====
        # Convertir NaN a string vac√≠o y asegurar que todo sea string
        if 'title' in self.df.columns:
            self.df['title_clean'] = self.df['title'].fillna('').astype(str)
        
        # ===== PASO 2: EXTRAER A√ëO DE PUBLICACI√ìN =====
        if 'publication_date' in self.df.columns:
            # Aplicar funci√≥n de extracci√≥n de a√±o a cada fecha
            self.df['year'] = self.df['publication_date'].apply(self._extract_year)
        else:
            # Si no hay columna de fecha, usar a√±o 0 para todos
            self.df['year'] = 0
            
        # ===== PASO 3: CREAR CLAVE DE ORDENAMIENTO =====
        # Tupla (a√±o, t√≠tulo_normalizado) para ordenamiento compuesto
        self.df['sort_key'] = self.df.apply(
            lambda row: (row.get('year', 0), row.get('title_clean', '').lower().strip()), 
            axis=1
        )
        
        print(f"üìä Datos preparados con {len(self.df)} registros v√°lidos")

    def _extract_year(self, date_str) -> int:
        """
        Extrae el a√±o (4 d√≠gitos) de una string de fecha.
        
        Busca un patr√≥n de 4 d√≠gitos que comience con 19 o 20 (a√±os 1900-2099)
        en la string de fecha proporcionada. Esto permite manejar m√∫ltiples
        formatos de fecha sin necesidad de parsing complejo.
        
        Args:
            date_str: String que contiene una fecha en cualquier formato.
                Ejemplos: "2023-01-15", "January 2023", "2023", "01/15/2023"
        
        Returns:
            int: A√±o de 4 d√≠gitos si se encuentra (1900-2099), 0 si no hay a√±o v√°lido.
        
        Algorithm:
            1. Verificar si es NaN o string vac√≠o ‚Üí retornar 0
            2. Buscar regex: \\b(19|20)\\d{2}\\b (a√±o de 4 d√≠gitos)
            3. Si se encuentra ‚Üí retornar como int
            4. Si no se encuentra ‚Üí retornar 0
        
        Example:
            >>> analyzer._extract_year("2023-01-15")
            2023
            >>> analyzer._extract_year("Published in 2023")
            2023
            >>> analyzer._extract_year("January 15, 2023")
            2023
            >>> analyzer._extract_year("No date here")
            0
            >>> analyzer._extract_year(None)
            0
            >>> analyzer._extract_year(pd.NaT)
            0
        
        Note:
            Solo encuentra a√±os entre 1900-2099. Fechas antiguas (ej: 1850)
            o futuras lejanas (ej: 2150) retornar√°n 0.
        """
        # Verificar si es NaN o vac√≠o
        if pd.isna(date_str) or date_str == '':
            return 0
            
        # Buscar patr√≥n de a√±o: 19xx o 20xx (4 d√≠gitos)
        # \b = word boundary (l√≠mite de palabra)
        # (19|20) = empieza con 19 o 20
        # \d{2} = seguido de dos d√≠gitos m√°s
        year_match = re.search(r'\b(19|20)\d{2}\b', str(date_str))
        
        if year_match:
            return int(year_match.group())
        
        return 0

    def _create_sortable_data(self) -> List[Tuple]:
        """
        Crea una lista de tuplas preparada para ordenamiento.
        
        Convierte el DataFrame en una lista de tuplas con el formato:
        (a√±o, t√≠tulo_normalizado, √≠ndice_original)
        
        Esta estructura es necesaria porque:
        1. Los algoritmos de ordenamiento operan sobre listas, no DataFrames
        2. Necesitamos mantener el √≠ndice original para reconstruir el DataFrame
        3. La tupla (a√±o, t√≠tulo) se ordena naturalmente en Python
        
        Returns:
            List[Tuple]: Lista de tuplas con formato (year, title, original_index).
                year (int): A√±o de publicaci√≥n
                title (str): T√≠tulo normalizado en min√∫sculas sin espacios extra
                original_index: √çndice de la fila en el DataFrame original
                
                Retorna lista vac√≠a si df es None o est√° vac√≠o.
        
        Example:
            >>> data = analyzer._create_sortable_data()
            >>> print(data[:3])
            [
                (2023, 'machine learning basics', 0),
                (2022, 'deep neural networks', 1),
                (2023, 'ai in healthcare', 2)
            ]
            
            >>> # Despu√©s de ordenar
            >>> sorted_data = sorted(data)
            >>> print(sorted_data[:3])
            [
                (2022, 'deep neural networks', 1),
                (2023, 'ai in healthcare', 2),
                (2023, 'machine learning basics', 0)
            ]
        
        Note:
            El √≠ndice original (tercer elemento) es cr√≠tico para poder
            reconstruir el DataFrame con _build_result_dataframe().
        """
        # Verificar que hay datos disponibles
        if self.df is None or self.df.empty:
            return []
            
        sortable_data = []
        
        # Iterar sobre cada fila del DataFrame
        for idx, row in self.df.iterrows():
            # Extraer a√±o (0 si no existe)
            year = row.get('year', 0)
            
            # Extraer y normalizar t√≠tulo (min√∫sculas, sin espacios extra)
            title = row.get('title_clean', '').lower().strip()
            
            # Crear tupla: (a√±o, t√≠tulo, √≠ndice_original)
            sortable_data.append((year, title, idx))
        
        return sortable_data

    def _build_result_dataframe(self, sorted_data: List[Tuple]) -> pd.DataFrame:
        """
        Reconstruye DataFrame a partir de datos ordenados.
        
        Toma la lista de tuplas ordenadas y reconstruye un DataFrame con las
        filas en el nuevo orden. Esto permite retornar el dataset completo
        ordenado, no solo una lista de tuplas.
        
        Args:
            sorted_data (List[Tuple]): Lista de tuplas ordenadas en formato
                (year, title, original_index). El tercer elemento (√≠ndice)
                se usa para recuperar las filas del DataFrame original.
        
        Returns:
            pd.DataFrame: DataFrame con todas las columnas originales, pero
                con las filas reordenadas seg√∫n sorted_data. Los √≠ndices se
                resetean a 0, 1, 2, ... N-1.
                
                Retorna DataFrame vac√≠o si self.df es None o est√° vac√≠o.
        
        Algorithm:
            1. Extraer √≠ndices originales del tercer elemento de cada tupla
            2. Usar iloc para obtener filas en el orden especificado
            3. Resetear √≠ndices para que sean secuenciales
        
        Example:
            >>> sorted_data = [(2022, 'title a', 5), (2023, 'title b', 2)]
            >>> result_df = analyzer._build_result_dataframe(sorted_data)
            >>> print(result_df.index.tolist())
            [0, 1]  # √çndices reseteados
            >>> # Las filas corresponden a los √≠ndices originales 5 y 2
        
        Note:
            El DataFrame resultante tiene los mismos datos que el original,
            solo cambia el orden de las filas.
        """
        # Verificar que hay DataFrame disponible
        if self.df is None or self.df.empty:
            return pd.DataFrame([])
            
        # Extraer √≠ndices originales (tercer elemento de cada tupla)
        sorted_indices = [item[2] for item in sorted_data if len(item) > 2]
        
        # Obtener filas en el orden especificado y resetear √≠ndices
        return self.df.iloc[sorted_indices].reset_index(drop=True)

    # ==================== FUNCIONES AUXILIARES PARA ALGORITMOS ====================

    def _heapify(self, arr, n, i):
        """
        Funci√≥n heapify para HeapSort - mantiene propiedad de heap m√°ximo.
        
        Reordena el sub√°rbol con ra√≠z en el √≠ndice i para mantener la propiedad
        de max-heap (padre >= hijos). Se usa tanto en la construcci√≥n inicial
        del heap como en la fase de extracci√≥n.
        
        Args:
            arr (list): Arreglo a heapificar (modificado in-place)
            n (int): Tama√±o del heap a considerar
            i (int): √çndice de la ra√≠z del sub√°rbol a heapificar
        
        Algorithm:
            1. Asumir que i es el m√°s grande
            2. Calcular √≠ndices de hijos: left = 2*i+1, right = 2*i+2
            3. Si hijo izquierdo > ra√≠z, actualizar √≠ndice del m√°s grande
            4. Si hijo derecho > m√°s grande actual, actualizar
            5. Si el m√°s grande no es i, intercambiar y heapificar recursivamente
        
        Complexity:
            Tiempo: O(log n) - altura del √°rbol
            Espacio: O(log n) - stack de recursi√≥n
        
        Example:
            >>> arr = [3, 5, 1, 4, 2]
            >>> analyzer._heapify(arr, 5, 0)
            # arr puede quedar como [5, 4, 1, 3, 2] dependiendo de la estructura
        """
        largest = i         # Inicializar largest como ra√≠z
        left = 2 * i + 1    # Hijo izquierdo = 2*i + 1
        right = 2 * i + 2   # Hijo derecho = 2*i + 2
        
        # Ver si hijo izquierdo existe y es mayor que ra√≠z
        if left < n and arr[left] > arr[largest]:
            largest = left
        
        # Ver si hijo derecho existe y es mayor que el m√°s grande actual
        if right < n and arr[right] > arr[largest]:
            largest = right
        
        # Si el m√°s grande no es la ra√≠z, intercambiar
        if largest != i:
            arr[i], arr[largest] = arr[largest], arr[i]
            
            # Heapificar recursivamente el sub√°rbol afectado
            self._heapify(arr, n, largest)

    def _quick_sort_partition(self, arr, low, high):
        """
        Funci√≥n de partici√≥n para QuickSort.
        
        Particiona el arreglo alrededor de un pivot (√∫ltimo elemento), colocando
        todos los elementos menores a la izquierda y mayores a la derecha.
        
        Args:
            arr (list): Arreglo a particionar (modificado in-place)
            low (int): √çndice inicial del segmento a particionar
            high (int): √çndice final del segmento (se usa como pivot)
        
        Returns:
            int: √çndice final del pivot despu√©s de la partici√≥n
        
        Algorithm (Esquema de Lomuto):
            1. Elegir pivot = arr[high] (√∫ltimo elemento)
            2. i = low - 1 (√≠ndice del elemento m√°s peque√±o)
            3. Para cada elemento j de low a high-1:
                Si arr[j] <= pivot:
                    Incrementar i
                    Intercambiar arr[i] con arr[j]
            4. Colocar pivot en su posici√≥n final: intercambiar arr[i+1] con arr[high]
            5. Retornar i+1 (posici√≥n del pivot)
        
        Example:
            >>> arr = [3, 1, 4, 1, 5, 9, 2, 6]
            >>> pi = analyzer._quick_sort_partition(arr, 0, 7)
            # arr queda particionado alrededor del pivot (6)
            # pi es la posici√≥n final del pivot
        """
        pivot = arr[high]  # Elegir √∫ltimo elemento como pivot
        i = low - 1        # √çndice del elemento m√°s peque√±o
        
        # Recorrer desde low hasta high-1
        for j in range(low, high):
            # Si elemento actual es menor o igual al pivot
            if arr[j] <= pivot:
                i += 1
                # Intercambiar arr[i] con arr[j]
                arr[i], arr[j] = arr[j], arr[i]
        
        # Colocar pivot en su posici√≥n correcta
        arr[i + 1], arr[high] = arr[high], arr[i + 1]
        return i + 1

    def _quick_sort_recursive(self, arr, low, high):
        """
        Funci√≥n recursiva principal de QuickSort.
        
        Implementa el algoritmo divide-y-conquista de QuickSort:
        1. Particionar el arreglo
        2. Ordenar recursivamente la parte izquierda
        3. Ordenar recursivamente la parte derecha
        
        Args:
            arr (list): Arreglo a ordenar (modificado in-place)
            low (int): √çndice inicial del segmento a ordenar
            high (int): √çndice final del segmento a ordenar
        
        Base Case:
            Si low >= high, el segmento tiene 0 o 1 elementos (ya ordenado)
        
        Example:
            >>> arr = [3, 1, 4, 1, 5, 9, 2, 6, 5]
            >>> analyzer._quick_sort_recursive(arr, 0, len(arr)-1)
            >>> print(arr)
            [1, 1, 2, 3, 4, 5, 5, 6, 9]
        """
        if low < high:
            # Particionar y obtener √≠ndice del pivot
            pi = self._quick_sort_partition(arr, low, high)
            
            # Ordenar elementos antes del pivot
            self._quick_sort_recursive(arr, low, pi - 1)
            
            # Ordenar elementos despu√©s del pivot
            self._quick_sort_recursive(arr, pi + 1, high)

    def _insert_tree_node(self, root, val):
        """
        Inserta un nuevo nodo en el √°rbol binario de b√∫squeda (BST).
        
        Inserta recursivamente un valor en el BST manteniendo la propiedad:
        - Valores menores van a la izquierda
        - Valores mayores o iguales van a la derecha
        
        Args:
            root (TreeNode): Ra√≠z del √°rbol o sub√°rbol
            val: Valor a insertar (t√≠picamente tupla (year, title, index))
        
        Returns:
            TreeNode: Nueva ra√≠z del √°rbol despu√©s de la inserci√≥n
        
        Example:
            >>> root = None
            >>> root = analyzer._insert_tree_node(root, (2023, "title a", 0))
            >>> root = analyzer._insert_tree_node(root, (2022, "title b", 1))
            >>> root = analyzer._insert_tree_node(root, (2024, "title c", 2))
            # Estructura del √°rbol:
            #       (2023, "title a", 0)
            #      /                    \\
            # (2022, "title b", 1)   (2024, "title c", 2)
        """
        # Caso base: si no hay nodo, crear uno nuevo
        if root is None:
            return TreeNode(val)
            
        # Insertar en sub√°rbol izquierdo si es menor
        if val < root.val:
            root.left = self._insert_tree_node(root.left, val)
        # Insertar en sub√°rbol derecho si es mayor o igual
        else:
            root.right = self._insert_tree_node(root.right, val)
            
        return root

    def _inorder_traversal(self, root, result):
        """
        Recorrido inorden (in-order traversal) del √°rbol binario.
        
        Recorre el √°rbol en orden: izquierda ‚Üí ra√≠z ‚Üí derecha.
        Este recorrido en un BST produce los elementos en orden ascendente.
        
        Args:
            root (TreeNode): Ra√≠z del √°rbol o sub√°rbol a recorrer
            result (list): Lista donde se acumulan los valores ordenados
                (modificada in-place)
        
        Algorithm:
            1. Recorrer sub√°rbol izquierdo recursivamente
            2. Visitar ra√≠z (agregar a result)
            3. Recorrer sub√°rbol derecho recursivamente
        
        Example:
            >>> # √Årbol:  5
            >>> #         / \\
            >>> #        3   7
            >>> result = []
            >>> analyzer._inorder_traversal(root, result)
            >>> print(result)
            [3, 5, 7]  # Orden ascendente
        """
        if root:
            # Primero recorrer sub√°rbol izquierdo
            self._inorder_traversal(root.left, result)
            
            # Luego visitar ra√≠z
            result.append(root.val)
            
            # Finalmente recorrer sub√°rbol derecho
            self._inorder_traversal(root.right, result)

    def _bitonic_merge(self, arr, low, cnt, up):
        """
        Funci√≥n merge para BitonicSort - fusiona secuencia bit√≥nica.
        
        Una secuencia bit√≥nica es aquella que primero crece y luego decrece
        (o viceversa). Este m√©todo fusiona recursivamente comparando elementos
        a distancia k y intercambi√°ndolos seg√∫n la direcci√≥n de ordenamiento.
        
        Args:
            arr (list): Arreglo a fusionar (modificado in-place)
            low (int): √çndice inicial del segmento
            cnt (int): N√∫mero de elementos en el segmento (debe ser potencia de 2)
            up (bool): Direcci√≥n de ordenamiento (True=ascendente, False=descendente)
        
        Note:
            BitonicSort es √∫til para procesamiento paralelo ya que las
            comparaciones en cada nivel pueden hacerse independientemente.
        """
        if cnt > 1:
            k = cnt // 2
            
            # Comparar y posiblemente intercambiar elementos a distancia k
            for i in range(low, low + k):
                # Intercambiar si (arr[i] > arr[i+k]) == up
                # Esto significa: si vamos ascendente y arr[i] > arr[i+k], intercambiar
                if (arr[i] > arr[i + k]) == up:
                    arr[i], arr[i + k] = arr[i + k], arr[i]
            
            # Recursivamente fusionar mitades
            self._bitonic_merge(arr, low, k, up)
            self._bitonic_merge(arr, low + k, k, up)

    def _bitonic_sort_recursive(self, arr, low, cnt, up):
        """
        Funci√≥n recursiva principal de BitonicSort.
        
        Crea recursivamente una secuencia bit√≥nica y luego la fusiona.
        El algoritmo requiere que el tama√±o del arreglo sea potencia de 2.
        
        Args:
            arr (list): Arreglo a ordenar (modificado in-place)
            low (int): √çndice inicial del segmento
            cnt (int): N√∫mero de elementos (debe ser potencia de 2)
            up (bool): Direcci√≥n de ordenamiento
        """
        if cnt > 1:
            k = cnt // 2
            
            # Ordenar primera mitad en orden ascendente
            self._bitonic_sort_recursive(arr, low, k, True)
            
            # Ordenar segunda mitad en orden descendente
            self._bitonic_sort_recursive(arr, low + k, k, False)
            
            # Fusionar toda la secuencia en direcci√≥n 'up'
            self._bitonic_merge(arr, low, cnt, up)

    def _binary_search_insertion(self, arr, val, start, end):
        """
        B√∫squeda binaria para encontrar posici√≥n de inserci√≥n.
        
        Encuentra la posici√≥n correcta donde insertar 'val' en arr[start:end+1]
        para mantener el orden. Usado por BinaryInsertionSort.
        
        Args:
            arr (list): Arreglo ordenado donde buscar posici√≥n
            val: Valor a insertar
            start (int): √çndice inicial de b√∫squeda
            end (int): √çndice final de b√∫squeda
        
        Returns:
            int: √çndice donde debe insertarse val
        
        Example:
            >>> arr = [1, 3, 5, 7, 9]
            >>> pos = analyzer._binary_search_insertion(arr, 6, 0, 4)
            >>> print(pos)
            3  # Debe insertarse entre 5 y 7
        """
        # Caso base 1: solo un elemento
        if start == end:
            return start if arr[start] > val else start + 1
            
        # Caso base 2: rango inv√°lido
        if start > end:
            return start
        
        # Buscar en mitad del rango
        mid = (start + end) // 2
        
        if arr[mid] < val:
            # val va en la mitad derecha
            return self._binary_search_insertion(arr, val, mid + 1, end)
        elif arr[mid] > val:
            # val va en la mitad izquierda
            return self._binary_search_insertion(arr, val, start, mid - 1)
        else:
            # Valor igual encontrado
            return mid

    def _counting_sort_for_radix(self, arr, exp):
        """
        Counting Sort estable para RadixSort seg√∫n un d√≠gito espec√≠fico.
        
        Ordena el arreglo seg√∫n el d√≠gito en la posici√≥n 'exp' (unidades,
        decenas, centenas, etc.) manteniendo estabilidad (orden relativo de
        elementos iguales se preserva).
        
        Args:
            arr (list): Arreglo de tuplas (year, title, index) a ordenar
            exp (int): Posici√≥n del d√≠gito (1=unidades, 10=decenas, 100=centenas)
        
        Algorithm:
            1. Contar ocurrencias de cada d√≠gito (0-9)
            2. Calcular posiciones acumulativas
            3. Construir arreglo de salida en orden estable
            4. Copiar de vuelta al arreglo original
        """
        n = len(arr)
        output = [None] * n  # Arreglo de salida
        count = [0] * 10     # Contador para d√≠gitos 0-9
        
        # ===== PASO 1: CONTAR OCURRENCIAS =====
        for i in range(n):
            # Extraer d√≠gito en posici√≥n exp del a√±o (primer elemento de tupla)
            index = (arr[i][0] // exp) % 10
            count[index] += 1
        
        # ===== PASO 2: CALCULAR POSICIONES ACUMULATIVAS =====
        for i in range(1, 10):
            count[i] += count[i - 1]
        
        # ===== PASO 3: CONSTRUIR ARREGLO DE SALIDA (desde el final para estabilidad) =====
        i = n - 1
        while i >= 0:
            index = (arr[i][0] // exp) % 10
            output[count[index] - 1] = arr[i]
            count[index] -= 1
            i -= 1
        
        # ===== PASO 4: COPIAR RESULTADO A ARREGLO ORIGINAL =====
        for i in range(n):
            arr[i] = output[i]

    # ==================== ALGORITMOS DE ORDENAMIENTO ====================

    def tim_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        TimSort - Algoritmo h√≠brido de ordenamiento usado por Python.
        
        TimSort combina Merge Sort e Insertion Sort, aprovechando las ventajas
        de ambos. Es el algoritmo usado internamente por Python en sorted() y
        list.sort(). Es estable y tiene excelente rendimiento en datos reales.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con:
                - DataFrame ordenado por (a√±o, t√≠tulo)
                - Tiempo de ejecuci√≥n en segundos
        
        Complexity:
            - Tiempo mejor caso: O(n) - datos casi ordenados
            - Tiempo promedio: O(n log n)
            - Tiempo peor caso: O(n log n)
            - Espacio: O(n)
        
        Advantages:
            - Extremadamente r√°pido en datos reales
            - Estable (preserva orden de elementos iguales)
            - Adaptativo (aprovecha orden existente)
            - Implementaci√≥n nativa optimizada en C
        
        Use Cases:
            - Datasets de cualquier tama√±o
            - Cuando se necesita estabilidad
            - Datos parcialmente ordenados
            - Uso general (mejor opci√≥n por defecto)
        
        Example:
            >>> df_sorted, time_taken = analyzer.tim_sort()
            >>> print(f"Ordenado en {time_taken*1000:.2f}ms")
            >>> print(df_sorted[['year', 'title']].head())
        """
        # Crear lista de tuplas ordenables
        data = self._create_sortable_data()
        
        # Medir tiempo de ejecuci√≥n
        start_time = time.perf_counter()
        
        # Python usa TimSort internamente en sorted()
        sorted_data = sorted(data, key=lambda x: (x[0], x[1]))
        
        end_time = time.perf_counter()
        
        # Reconstruir DataFrame con orden nuevo
        result_df = self._build_result_dataframe(sorted_data)
        return result_df, end_time - start_time

    def comb_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        CombSort - Mejora de BubbleSort con gap decreciente.
        
        CombSort mejora BubbleSort usando un "gap" (distancia entre elementos
        comparados) que decrece gradualmente con factor de reducci√≥n 1.3.
        Elimina "tortugas" (valores peque√±os al final) m√°s r√°pido que BubbleSort.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo promedio: O(n¬≤/2^p) donde p es n√∫mero de incrementos
            - Tiempo peor caso: O(n¬≤)
            - Espacio: O(1) - in-place
        
        Algorithm:
            1. Empezar con gap = n
            2. Reducir gap por factor 1.3 en cada iteraci√≥n
            3. Comparar elementos a distancia gap
            4. Intercambiar si est√°n en orden incorrecto
            5. Repetir hasta gap = 1 y no haya m√°s intercambios
        
        Advantages:
            - M√°s r√°pido que BubbleSort
            - Simple de implementar
            - In-place (no usa memoria extra)
        
        Disadvantages:
            - Sigue siendo O(n¬≤) en peor caso
            - No estable
            - Superado por algoritmos O(n log n)
        
        Example:
            >>> df_sorted, time_taken = analyzer.comb_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        n = len(data)
        gap = n              # Gap inicial = tama√±o del arreglo
        shrink = 1.3         # Factor de reducci√≥n √≥ptimo
        sorted_flag = False  # Indica si hay que seguir iterando
        
        while not sorted_flag:
            # Reducir gap
            gap = int(gap / shrink)
            if gap <= 1:
                gap = 1
                sorted_flag = True  # √öltima pasada con gap=1
            
            i = 0
            # Comparar elementos a distancia gap
            while i + gap < n:
                if data[i] > data[i + gap]:
                    # Intercambiar elementos desordenados
                    data[i], data[i + gap] = data[i + gap], data[i]
                    sorted_flag = False  # Hubo cambio, seguir iterando
                i += 1
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def selection_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        SelectionSort - Algoritmo simple de selecci√≥n del m√≠nimo.
        
        En cada iteraci√≥n, encuentra el elemento m√≠nimo del segmento no ordenado
        y lo intercambia con el primer elemento de ese segmento. Simple pero
        ineficiente para datasets grandes.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo todos los casos: O(n¬≤)
            - Espacio: O(1) - in-place
            - Comparaciones: n(n-1)/2 siempre
            - Intercambios: O(n) - solo uno por iteraci√≥n
        
        Algorithm:
            1. Para cada posici√≥n i de 0 a n-1:
               a. Encontrar √≠ndice del m√≠nimo en arr[i:n]
               b. Intercambiar arr[i] con el m√≠nimo
        
        Advantages:
            - Muy simple de entender e implementar
            - M√≠nimo n√∫mero de intercambios: O(n)
            - In-place (no usa memoria extra)
        
        Disadvantages:
            - O(n¬≤) siempre, incluso si ya est√° ordenado
            - No estable en implementaci√≥n b√°sica
            - Muy lento para datasets >5000 elementos
        
        Use Cases:
            - Datasets muy peque√±os (<100 elementos)
            - Cuando el costo de intercambios es alto
            - Prop√≥sitos educativos
        
        Example:
            >>> df_sorted, time_taken = analyzer.selection_sort()
            # Para 1000 elementos: ~100-200ms
            # Para 10000 elementos: ~10-20 segundos
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        n = len(data)
        # Para cada posici√≥n en el arreglo
        for i in range(n):
            min_idx = i  # Asumir que el m√≠nimo es el elemento actual
            
            # Buscar el m√≠nimo en el resto del arreglo
            for j in range(i + 1, n):
                if data[j] < data[min_idx]:
                    min_idx = j
            
            # Intercambiar el m√≠nimo encontrado con el elemento en posici√≥n i
            data[i], data[min_idx] = data[min_idx], data[i]
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def tree_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        TreeSort - Ordenamiento mediante √°rbol binario de b√∫squeda (BST).
        
        Construye un √°rbol binario de b√∫squeda insertando todos los elementos,
        luego realiza un recorrido inorden para obtener los elementos ordenados.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo promedio: O(n log n) - √°rbol balanceado
            - Tiempo peor caso: O(n¬≤) - √°rbol desbalanceado (datos ordenados)
            - Espacio: O(n) - nodos del √°rbol
        
        Algorithm:
            1. Crear √°rbol vac√≠o (root = None)
            2. Insertar cada elemento en el BST
            3. Recorrer √°rbol inorden (izquierda ‚Üí ra√≠z ‚Üí derecha)
            4. El recorrido produce elementos en orden ascendente
        
        Advantages:
            - O(n log n) en promedio
            - √ötil si necesitas mantener el √°rbol despu√©s
            - F√°cil de entender conceptualmente
        
        Disadvantages:
            - O(n¬≤) en peor caso (datos ya ordenados)
            - Usa O(n) espacio adicional
            - No estable
            - Puede causar stack overflow por recursi√≥n profunda
        
        Use Cases:
            - Cuando necesitas el BST para otras operaciones
            - Datasets con datos aleatorios
            - Prop√≥sitos educativos
        
        Example:
            >>> df_sorted, time_taken = analyzer.tree_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        # Construir √°rbol binario de b√∫squeda
        root = None
        for item in data:
            root = self._insert_tree_node(root, item)
        
        # Recorrer √°rbol inorden para obtener elementos ordenados
        sorted_data = []
        self._inorder_traversal(root, sorted_data)
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(sorted_data)
        return result_df, end_time - start_time

    def pigeonhole_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        PigeonholeSort - Ordenamiento por distribuci√≥n en casilleros.
        
        Distribuye elementos en "casilleros" (pigeonholes) seg√∫n su valor (a√±o),
        luego ordena cada casillero y concatena. Eficiente cuando el rango de
        valores es peque√±o comparado con el n√∫mero de elementos.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo: O(n + k) donde k = rango de a√±os
            - Espacio: O(n + k)
        
        Algorithm:
            1. Encontrar rango de a√±os: [min_year, max_year]
            2. Crear k casilleros (uno por cada a√±o posible)
            3. Distribuir elementos por a√±o en casilleros
            4. Ordenar cada casillero por t√≠tulo
            5. Concatenar casilleros en orden
        
        Advantages:
            - O(n + k) muy r√°pido si k es peque√±o
            - Simple de implementar
            - Estable si se implementa correctamente
        
        Disadvantages:
            - Requiere conocer el rango de valores
            - Ineficiente si k >> n
            - Usa O(k) espacio adicional
        
        Use Cases:
            - Datos con rango peque√±o y conocido (a√±os, calificaciones)
            - Cuando k ‚âà n
            - Distribuciones uniformes
        
        Example:
            >>> df_sorted, time_taken = analyzer.pigeonhole_sort()
            # Para datos de 2000-2025 (k=25): muy r√°pido
            # Para datos de 1800-2025 (k=225): menos eficiente
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        # Manejar caso de datos vac√≠os
        if not data:
            end_time = time.perf_counter()
            return (self.df.copy() if isinstance(self.df, pd.DataFrame) else pd.DataFrame([]), 
                    end_time - start_time)
        
        # Extraer a√±os y calcular rango
        years = [item[0] for item in data]
        min_year = min(years)
        max_year = max(years)
        range_years = max_year - min_year + 1
        
        # Crear casilleros (pigeonholes) - uno por cada a√±o posible
        pigeonholes = [[] for _ in range(range_years)]
        
        # Distribuir elementos en casilleros seg√∫n a√±o
        for item in data:
            year_idx = item[0] - min_year  # √çndice del casillero
            pigeonholes[year_idx].append(item)
        
        # Ordenar cada casillero por t√≠tulo y concatenar
        sorted_data = []
        for hole in pigeonholes:
            hole.sort(key=lambda x: x[1])  # Ordenar por t√≠tulo
            sorted_data.extend(hole)
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(sorted_data)
        return result_df, end_time - start_time

    def bucket_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        BucketSort - Ordenamiento por distribuci√≥n en cubetas.
        
        Similar a PigeonholeSort, pero usa a√±os √∫nicos como buckets en lugar
        de crear un bucket por cada a√±o posible. M√°s eficiente en memoria
        para rangos grandes con gaps.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo promedio: O(n + k) donde k = n√∫mero de a√±os √∫nicos
            - Tiempo peor caso: O(n¬≤) si todos en un bucket
            - Espacio: O(n + k)
        
        Algorithm:
            1. Identificar a√±os √∫nicos en los datos
            2. Crear un bucket por cada a√±o √∫nico
            3. Distribuir elementos en buckets seg√∫n a√±o
            4. Ordenar cada bucket por t√≠tulo
            5. Concatenar buckets en orden de a√±o
        
        Advantages:
            - O(n + k) si distribuci√≥n uniforme
            - Usa menos memoria que PigeonholeSort para rangos con gaps
            - Flexible con distribuci√≥n de valores
        
        Disadvantages:
            - O(n¬≤) si distribuci√≥n muy desigual
            - Requiere ordenar cada bucket
        
        Differences from PigeonholeSort:
            - PigeonholeSort: bucket por cada valor posible en rango
            - BucketSort: bucket solo por valores que existen
            
            Ejemplo: a√±os [2000, 2005, 2025]
            - PigeonholeSort: 26 buckets (2000-2025)
            - BucketSort: 3 buckets (2000, 2005, 2025)
        
        Example:
            >>> df_sorted, time_taken = analyzer.bucket_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        # Manejar caso de datos vac√≠os
        if not data:
            end_time = time.perf_counter()
            return (self.df.copy() if isinstance(self.df, pd.DataFrame) else pd.DataFrame([]), 
                    end_time - start_time)
        
        # Obtener a√±os √∫nicos presentes en los datos
        years = set(item[0] for item in data)
        
        # Crear diccionario de buckets - uno por a√±o √∫nico
        year_buckets = {year: [] for year in years}
        
        # Distribuir elementos en buckets seg√∫n a√±o
        for item in data:
            year_buckets[item[0]].append(item)
        
        # Ordenar cada bucket por t√≠tulo y concatenar en orden de a√±o
        sorted_data = []
        for year in sorted(years):  # Procesar a√±os en orden ascendente
            bucket = year_buckets[year]
            bucket.sort(key=lambda x: x[1])  # Ordenar por t√≠tulo
            sorted_data.extend(bucket)
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(sorted_data)
        return result_df, end_time - start_time

    def quick_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        QuickSort - Algoritmo divide-y-conquista cl√°sico.
        
        Uno de los algoritmos m√°s r√°pidos en la pr√°ctica. Usa estrategia
        divide-y-conquista: particiona el arreglo alrededor de un pivot y
        ordena recursivamente las partes.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo promedio: O(n log n)
            - Tiempo peor caso: O(n¬≤) - datos ordenados con pivot mal elegido
            - Espacio: O(log n) - stack de recursi√≥n
        
        Algorithm:
            1. Elegir pivot (√∫ltimo elemento en esta implementaci√≥n)
            2. Particionar: menores a la izquierda, mayores a la derecha
            3. Ordenar recursivamente parte izquierda
            4. Ordenar recursivamente parte derecha
        
        Advantages:
            - Muy r√°pido en promedio: O(n log n)
            - In-place: usa O(log n) espacio
            - Cache-friendly: buena localidad de referencia
            - Usado ampliamente en sistemas de producci√≥n
        
        Disadvantages:
            - O(n¬≤) en peor caso (raro con pivot aleatorio)
            - No estable en implementaci√≥n est√°ndar
            - Recursi√≥n profunda puede causar stack overflow
        
        Improvements:
            - Elegir pivot aleatorio o mediana de tres
            - Cambiar a InsertionSort para subarreglos peque√±os
            - Implementaci√≥n iterativa para evitar stack overflow
        
        Example:
            >>> df_sorted, time_taken = analyzer.quick_sort()
            # T√≠picamente uno de los m√°s r√°pidos junto con TimSort
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        # Ordenar in-place mediante recursi√≥n
        self._quick_sort_recursive(data, 0, len(data) - 1)
        end_time = time.perf_counter()
        
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def heap_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        HeapSort - Ordenamiento mediante heap binario.
        
        Construye un max-heap del arreglo, luego extrae repetidamente el
        m√°ximo (ra√≠z) y lo coloca al final. Garantiza O(n log n) en todos
        los casos.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo todos los casos: O(n log n)
            - Espacio: O(1) - in-place
        
        Algorithm:
            FASE 1: Construir Max-Heap
            1. Para cada nodo desde n/2-1 hasta 0:
               Heapificar sub√°rbol con ra√≠z en ese nodo
            
            FASE 2: Extraer Elementos
            2. Para i desde n-1 hasta 1:
               a. Intercambiar arr[0] (m√°ximo) con arr[i]
               b. Heapificar arr[0:i] para restaurar max-heap
        
        Advantages:
            - O(n log n) garantizado (sin peor caso O(n¬≤))
            - In-place: O(1) espacio adicional
            - No requiere recursi√≥n (puede ser iterativo)
            - √ötil cuando memoria es limitada
        
        Disadvantages:
            - No estable
            - Constante mayor que QuickSort en promedio
            - Pobre localidad de cache
        
        Use Cases:
            - Cuando se requiere O(n log n) garantizado
            - Memoria limitada (in-place)
            - Sistemas en tiempo real (tiempo predecible)
        
        Example:
            >>> df_sorted, time_taken = analyzer.heap_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        n = len(data)
        
        # ===== FASE 1: CONSTRUIR MAX-HEAP =====
        # Heapificar desde el √∫ltimo nodo interno hasta la ra√≠z
        for i in range(n // 2 - 1, -1, -1):
            self._heapify(data, n, i)
        
        # ===== FASE 2: EXTRAER ELEMENTOS UNO POR UNO =====
        for i in range(n - 1, 0, -1):
            # Mover ra√≠z actual (m√°ximo) al final
            data[0], data[i] = data[i], data[0]
            
            # Heapificar el heap reducido
            self._heapify(data, i, 0)
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def bitonic_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        BitonicSort - Ordenamiento para procesamiento paralelo.
        
        Algoritmo dise√±ado para hardware paralelo. Construye una secuencia
        bit√≥nica (primero crece, luego decrece) y la fusiona. Requiere que
        el tama√±o del arreglo sea potencia de 2.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo: O(n log¬≤n) - m√°s lento que O(n log n)
            - Espacio: O(n) - puede necesitar padding
            - Pero: Altamente paralelizable
        
        Algorithm:
            1. Extender arreglo a siguiente potencia de 2 (rellenar con valores grandes)
            2. Construir secuencia bit√≥nica recursivamente:
               - Primera mitad ordenar ascendente
               - Segunda mitad ordenar descendente
            3. Fusionar secuencia bit√≥nica
            4. Remover padding agregado
        
        Advantages:
            - Excelente para GPUs y hardware paralelo
            - Todas las comparaciones en cada nivel son independientes
            - Determin√≠stico y predecible
        
        Disadvantages:
            - O(n log¬≤n) m√°s lento que O(n log n) en secuencial
            - Requiere tama√±o potencia de 2
            - Complejo de implementar correctamente
        
        Use Cases:
            - Procesamiento paralelo en GPU
            - Hardware especializado (FPGAs)
            - Cuando se requiere alta paralelizaci√≥n
        
        Example:
            >>> df_sorted, time_taken = analyzer.bitonic_sort()
            # M√°s lento en CPU secuencial, pero √∫til para demostraci√≥n
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        # ===== PASO 1: EXTENDER A POTENCIA DE 2 =====
        n = len(data)
        # Calcular siguiente potencia de 2
        next_power_of_2 = 1 << (n - 1).bit_length()
        
        # Llenar con elementos muy grandes (ir√°n al final)
        max_item = (9999, 'zzzzz', -1)
        while len(data) < next_power_of_2:
            data.append(max_item)
        
        # ===== PASO 2: ORDENAR BIT√ìNICAMENTE =====
        self._bitonic_sort_recursive(data, 0, len(data), True)
        
        # ===== PASO 3: REMOVER PADDING =====
        data = data[:n]
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def gnome_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        GnomeSort - Algoritmo simple similar a InsertionSort.
        
        Tambi√©n conocido como "Stupid Sort". Similar a InsertionSort pero m√°s
        simple: compara elementos adyacentes, si est√°n en orden correcto avanza,
        si no los intercambia y retrocede.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo mejor caso: O(n) - datos ya ordenados
            - Tiempo promedio: O(n¬≤)
            - Tiempo peor caso: O(n¬≤)
            - Espacio: O(1) - in-place
        
        Algorithm:
            1. Empezar en posici√≥n 0
            2. Si est√° en inicio o elemento >= anterior:
               Avanzar una posici√≥n
            3. Si elemento < anterior:
               Intercambiar con anterior
               Retroceder una posici√≥n
            4. Repetir hasta llegar al final
        
        Advantages:
            - Extremadamente simple de implementar
            - O(n) si ya est√° casi ordenado
            - Estable
            - In-place
        
        Disadvantages:
            - O(n¬≤) en promedio y peor caso
            - Muy lento para datasets grandes
            - No tiene ventajas sobre InsertionSort
        
        Use Cases:
            - Prop√≥sitos educativos
            - Datasets muy peque√±os
            - Cuando simplicidad es m√°s importante que eficiencia
        
        Example:
            >>> df_sorted, time_taken = analyzer.gnome_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        index = 0
        n = len(data)
        
        while index < n:
            # Si estamos en el inicio o el elemento actual >= anterior
            if index == 0 or data[index] >= data[index - 1]:
                index += 1  # Avanzar
            else:
                # Elemento actual < anterior: intercambiar y retroceder
                data[index], data[index - 1] = data[index - 1], data[index]
                index -= 1
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def binary_insertion_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        BinaryInsertionSort - InsertionSort optimizado con b√∫squeda binaria.
        
        Mejora InsertionSort usando b√∫squeda binaria para encontrar la posici√≥n
        de inserci√≥n, reduciendo comparaciones de O(n¬≤) a O(n log n). Sin embargo,
        los movimientos siguen siendo O(n¬≤).
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Comparaciones: O(n log n) - b√∫squeda binaria
            - Movimientos: O(n¬≤) - shift de elementos
            - Tiempo total: O(n¬≤) - dominado por movimientos
            - Espacio: O(1) - in-place
        
        Algorithm:
            1. Para cada elemento i desde 1 hasta n-1:
               a. Usar b√∫squeda binaria para encontrar posici√≥n correcta en arr[0:i]
               b. Desplazar elementos hacia la derecha
               c. Insertar elemento en posici√≥n encontrada
        
        Advantages:
            - Menos comparaciones que InsertionSort est√°ndar
            - Estable
            - In-place
            - O(n) en mejor caso (datos ordenados)
        
        Disadvantages:
            - Sigue siendo O(n¬≤) debido a movimientos
            - No mejora significativamente el rendimiento total
            - M√°s complejo que InsertionSort est√°ndar
        
        Use Cases:
            - Cuando comparaciones son caras pero movimientos baratos
            - Prop√≥sitos educativos
            - Datasets peque√±os a medianos
        
        Example:
            >>> df_sorted, time_taken = analyzer.binary_insertion_sort()
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        n = len(data)
        
        # Para cada elemento desde el segundo hasta el √∫ltimo
        for i in range(1, n):
            key = data[i]
            
            # Encontrar posici√≥n de inserci√≥n usando b√∫squeda binaria
            pos = self._binary_search_insertion(data, key, 0, i - 1)
            
            # Desplazar elementos hacia la derecha para hacer espacio
            for j in range(i - 1, pos - 1, -1):
                data[j + 1] = data[j]
            
            # Insertar elemento en su posici√≥n correcta
            data[pos] = key
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    def radix_sort(self) -> Tuple[pd.DataFrame, float]:
        """
        RadixSort - Ordenamiento por d√≠gitos/caracteres.
        
        Ordena n√∫meros procesando d√≠gito por d√≠gito desde el menos significativo
        (unidades) hasta el m√°s significativo (miles). Usa CountingSort estable
        como subrutina para cada d√≠gito.
        
        Returns:
            Tuple[pd.DataFrame, float]: Tupla con DataFrame ordenado y tiempo.
        
        Complexity:
            - Tiempo: O(d * (n + k)) donde:
              d = n√∫mero de d√≠gitos
              n = n√∫mero de elementos
              k = rango de valores (0-9 para d√≠gitos)
            - Espacio: O(n + k)
        
        Algorithm:
            1. Encontrar n√∫mero m√°ximo de d√≠gitos (a√±o m√°s grande)
            2. Para cada posici√≥n de d√≠gito (1, 10, 100, 1000):
               a. Ordenar por ese d√≠gito usando CountingSort estable
            3. Ordenar por t√≠tulo dentro de cada grupo de a√±o
        
        Advantages:
            - O(d*n) para enteros de tama√±o fijo
            - Estable
            - Puede ser muy r√°pido para enteros peque√±os
        
        Disadvantages:
            - Solo funciona con enteros o strings
            - Ineficiente si d es grande
            - Usa memoria adicional O(n)
            - Implementaci√≥n m√°s compleja
        
        Use Cases:
            - Ordenar enteros de tama√±o fijo (c√≥digos postales, fechas)
            - Cuando d es peque√±o comparado con log n
            - Necesidad de estabilidad
        
        Example:
            >>> df_sorted, time_taken = analyzer.radix_sort()
            # Para a√±os (4 d√≠gitos): d=4, muy eficiente
        """
        data = self._create_sortable_data()
        
        start_time = time.perf_counter()
        
        # Manejar caso de datos vac√≠os
        if not data:
            end_time = time.perf_counter()
            return (self.df.copy() if isinstance(self.df, pd.DataFrame) else pd.DataFrame([]), 
                    end_time - start_time)
        
        # ===== PASO 1: OBTENER A√ëO M√ÅXIMO =====
        max_year = max(item[0] for item in data) if data else 0
        
        # ===== PASO 2: ORDENAR POR CADA D√çGITO =====
        # Empezar con exp=1 (unidades), luego 10 (decenas), 100 (centenas), etc.
        exp = 1
        while max_year // exp > 0:
            self._counting_sort_for_radix(data, exp)
            exp *= 10
        
        # ===== PASO 3: ORDENAR POR T√çTULO DENTRO DE CADA A√ëO =====
        current_year = None
        year_group_start = 0
        
        for i in range(len(data)):
            if data[i][0] != current_year:
                # Nuevo a√±o encontrado: ordenar grupo anterior por t√≠tulo
                if current_year is not None:
                    year_group = data[year_group_start:i]
                    year_group.sort(key=lambda x: x[1])
                    data[year_group_start:i] = year_group
                
                current_year = data[i][0]
                year_group_start = i
        
        # Ordenar √∫ltimo grupo
        if len(data) > year_group_start:
            year_group = data[year_group_start:]
            year_group.sort(key=lambda x: x[1])
            data[year_group_start:] = year_group
        
        end_time = time.perf_counter()
        result_df = self._build_result_dataframe(data)
        return result_df, end_time - start_time

    # ==================== AN√ÅLISIS Y VISUALIZACI√ìN ====================

    def run_all_algorithms(self) -> Dict[str, Tuple[pd.DataFrame, float]]:
        """
        Ejecuta todos los 12 algoritmos de ordenamiento y mide su rendimiento.
        
        Este es el m√©todo principal para benchmarking. Ejecuta cada algoritmo
        secuencialmente, mide su tiempo de ejecuci√≥n con precisi√≥n, y captura
        cualquier error que ocurra.
        
        Returns:
            Dict[str, Tuple[pd.DataFrame, float]]: Diccionario donde:
                - Key: Nombre del algoritmo
                - Value: Tupla (DataFrame ordenado, tiempo en segundos)
                         o (None, float('inf')) si hubo error
        
        Algorithms Executed:
            1. TimSort
            2. CombSort
            3. SelectionSort
            4. TreeSort
            5. PigeonholeSort
            6. BucketSort
            7. QuickSort
            8. HeapSort
            9. BitonicSort
            10. GnomeSort
            11. BinaryInsertionSort
            12. RadixSort
        
        Output:
            Imprime progreso en consola:
            - ‚è≥ Mensaje de inicio para cada algoritmo
            - ‚úÖ Tiempo de ejecuci√≥n en ms si exitoso
            - ‚ùå Mensaje de error si falla
        
        Example:
            >>> results = analyzer.run_all_algorithms()
            üöÄ Ejecutando todos los algoritmos de ordenamiento...
            ‚è≥ Ejecutando TimSort...
            ‚úÖ TimSort completado en 12.345ms
            ‚è≥ Ejecutando CombSort...
            ‚úÖ CombSort completado en 45.678ms
            ...
            
            >>> # Analizar resultados
            >>> for name, (df, time_taken) in results.items():
            ...     if time_taken != float('inf'):
            ...         print(f"{name}: {time_taken*1000:.2f}ms")
        
        Note:
            Los algoritmos se ejecutan en el orden listado. Si alguno falla,
            los dem√°s contin√∫an ejecut√°ndose.
        """
        print("üöÄ Ejecutando todos los algoritmos de ordenamiento...")
        
        # Lista de tuplas (nombre, funci√≥n)
        algorithms = [
            ("TimSort", self.tim_sort),
            ("CombSort", self.comb_sort),
            ("SelectionSort", self.selection_sort),
            ("TreeSort", self.tree_sort),
            ("PigeonholeSort", self.pigeonhole_sort),
            ("BucketSort", self.bucket_sort),
            ("QuickSort", self.quick_sort),
            ("HeapSort", self.heap_sort),
            ("BitonicSort", self.bitonic_sort),
            ("GnomeSort", self.gnome_sort),
            ("BinaryInsertionSort", self.binary_insertion_sort),
            ("RadixSort", self.radix_sort),
        ]
        
        results = {}
        
        # Ejecutar cada algoritmo
        for name, algorithm in algorithms:
            try:
                print(f"‚è≥ Ejecutando {name}...")
                result_df, exec_time = algorithm()
                results[name] = (result_df, exec_time)
                print(f"‚úÖ {name} completado en {exec_time*1000:.3f}ms")
            except Exception as e:
                # Capturar errores y continuar con el resto
                print(f"‚ùå Error en {name}: {e}")
                results[name] = (None, float('inf'))
        
        return results

    def create_time_comparison_chart(self, results: Dict[str, Tuple[pd.DataFrame, float]], 
                                   save_path: str = "sorting_times_comparison.png"):
        """
        Crea gr√°fico de barras comparando tiempos de ejecuci√≥n de algoritmos.
        
        Genera una visualizaci√≥n profesional con matplotlib mostrando los
        tiempos de ejecuci√≥n de cada algoritmo ordenados de m√°s r√°pido a
        m√°s lento. Incluye colores degradados y valores anotados.
        
        Args:
            results (Dict[str, Tuple[pd.DataFrame, float]]): Resultados de
                run_all_algorithms(). Debe contener tiempos de ejecuci√≥n.
            save_path (str, optional): Ruta donde guardar la imagen PNG.
                Por defecto "sorting_times_comparison.png".
        
        Chart Features:
            - Barras ordenadas ascendente por tiempo (m√°s r√°pido primero)
            - Colores degradados usando colormap 'viridis'
            - Valores de tiempo anotados sobre cada barra
            - Grid horizontal para facilitar lectura
            - Tama√±o: 14x8 pulgadas
            - Resoluci√≥n: 300 DPI
        
        Example:
            >>> results = analyzer.run_all_algorithms()
            >>> analyzer.create_time_comparison_chart(results)
            üìä Gr√°fico guardado en: sorting_times_comparison.png
            
            >>> # Personalizar nombre de archivo
            >>> analyzer.create_time_comparison_chart(
            ...     results,
            ...     save_path="benchmark_results_2025.png"
            ... )
        
        Note:
            El gr√°fico se muestra autom√°ticamente con plt.show() y tambi√©n
            se guarda en disco. Los algoritmos que fallaron (tiempo=inf)
            se excluyen del gr√°fico.
        """
        # ===== PASO 1: EXTRAER NOMBRES Y TIEMPOS V√ÅLIDOS =====
        names = []
        times = []
        
        for name, (df, time_taken) in results.items():
            # Solo incluir algoritmos que ejecutaron exitosamente
            if time_taken != float('inf'):
                names.append(name)
                times.append(time_taken * 1000)  # Convertir a milisegundos
        
        # ===== PASO 2: ORDENAR POR TIEMPO ASCENDENTE =====
        sorted_data = sorted(zip(names, times), key=lambda x: x[1])
        names, times = zip(*sorted_data)
        
        # ===== PASO 3: CREAR GR√ÅFICO =====
        plt.figure(figsize=(14, 8))
        
        # Obtener colormap viridis (degradado azul-verde-amarillo)
        cmap = cm.get_cmap('viridis')
        colors = cmap(np.linspace(0, 1, len(names))) if len(names) else []
        
        # Crear barras con colores degradados
        bars = plt.bar(range(len(names)), times, color=colors)
        
        # ===== PASO 4: CONFIGURAR EJES Y T√çTULOS =====
        plt.xlabel('Algoritmos de Ordenamiento', fontsize=12)
        plt.ylabel('Tiempo de Ejecuci√≥n (ms)', fontsize=12)
        plt.title('Comparaci√≥n de Tiempos de Ejecuci√≥n - Algoritmos de Ordenamiento\n(Productos Acad√©micos)', 
                  fontsize=14)
        plt.xticks(range(len(names)), names, rotation=45, ha='right')
        
        # ===== PASO 5: ANOTAR VALORES SOBRE BARRAS =====
        for i, (bar, time_val) in enumerate(zip(bars, times)):
            # Calcular posici√≥n del texto (encima de la barra)
            plt.text(bar.get_x() + bar.get_width()/2., 
                    bar.get_height() + max(times)*0.01,
                    f'{time_val:.2f}ms', 
                    ha='center', va='bottom', fontsize=9)
        
        # ===== PASO 6: AGREGAR GRID Y AJUSTAR LAYOUT =====
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        
        # ===== PASO 7: GUARDAR Y MOSTRAR =====
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"üìä Gr√°fico guardado en: {save_path}")

    def get_top_authors(self, top_n: int = 15) -> pd.DataFrame:
        """
        Obtiene los autores m√°s frecuentes en el dataset.
        
        Analiza la columna 'authors' del DataFrame, separa autores individuales,
        cuenta sus apariciones y retorna los top N m√°s frecuentes. √ötil para
        identificar autores prol√≠ficos o colaboradores frecuentes.
        
        Args:
            top_n (int, optional): N√∫mero de autores a retornar en el top.
                Por defecto 15.
        
        Returns:
            pd.DataFrame: DataFrame con dos columnas:
                - 'Autor': Nombre del autor
                - 'Apariciones': N√∫mero de veces que aparece
                
                Ordenado descendente por apariciones.
                Retorna DataFrame vac√≠o si no hay datos o columna 'authors'.
        
        Algorithm:
            1. Iterar sobre todos los registros
            2. Para cada registro, separar autores por ';'
            3. Limpiar espacios y agregar a lista
            4. Contar apariciones con Counter
            5. Obtener top N m√°s comunes
            6. Crear DataFrame con resultados
        
        Example:
            >>> top_authors = analyzer.get_top_authors(10)
            üìù Analizando top 10 autores...
            ‚úÖ Top 10 autores identificados
            >>> print(top_authors)
                Autor              Apariciones
            0   John Smith         45
            1   Jane Doe           38
            2   Michael Johnson    32
            ...
            
            >>> # Exportar resultados
            >>> top_authors.to_csv("top_authors.csv", index=False)
        
        Use Cases:
            - Identificar investigadores prol√≠ficos
            - An√°lisis de redes de colaboraci√≥n
            - Bibliometr√≠a y an√°lisis de citas
            - Informes de productividad acad√©mica
        
        Note:
            Asume que los autores est√°n separados por ';' en la columna 'authors'.
            Modifica el separador si tu dataset usa otro formato.
        """
        print(f"üìù Analizando top {top_n} autores...")
        
        all_authors = []
        
        # Verificar que hay datos y columna 'authors'
        if not isinstance(self.df, pd.DataFrame) or self.df.empty or 'authors' not in self.df.columns:
            return pd.DataFrame(columns=['Autor','Apariciones'])

        # ===== PASO 1: EXTRAER TODOS LOS AUTORES =====
        for authors_str in self.df['authors']:
            # Verificar que no es NaN ni vac√≠o
            if pd.notna(authors_str) and authors_str != '':
                # Dividir por punto y coma (separador de autores)
                authors_list = str(authors_str).split(';')
                
                # Limpiar y agregar cada autor
                for author in authors_list:
                    cleaned_author = author.strip()
                    if cleaned_author:
                        all_authors.append(cleaned_author)
        
        # ===== PASO 2: CONTAR APARICIONES =====
        author_counts = Counter(all_authors)
        
        # ===== PASO 3: OBTENER TOP N =====
        top_authors = author_counts.most_common(top_n)
        
        # ===== PASO 4: CREAR DATAFRAME =====
        top_authors_df = pd.DataFrame(top_authors, columns=['Autor', 'Apariciones'])
        
        print(f"‚úÖ Top {len(top_authors_df)} autores identificados")
        
        return top_authors_df

    def save_sorted_results(self, results: Dict[str, Tuple[pd.DataFrame, float]], 
                           output_dir: str = "sorted_results"):
        """
        Guarda los resultados ordenados de cada algoritmo en archivos CSV.
        
        Crea un directorio y guarda un archivo CSV por cada algoritmo que
        ejecut√≥ exitosamente. √ötil para inspeccionar resultados individuales
        o verificar que todos los algoritmos ordenan correctamente.
        
        Args:
            results (Dict[str, Tuple[pd.DataFrame, float]]): Resultados de
                run_all_algorithms().
            output_dir (str, optional): Directorio donde crear los archivos.
                Se crea si no existe. Por defecto "sorted_results".
        
        Files Created:
            - {output_dir}/TimSort_sorted.csv
            - {output_dir}/QuickSort_sorted.csv
            - {output_dir}/HeapSort_sorted.csv
            - ... (uno por cada algoritmo)
        
        Example:
            >>> results = analyzer.run_all_algorithms()
            >>> analyzer.save_sorted_results(results)
            üíæ Guardando resultados en directorio: sorted_results
            ‚úÖ TimSort: guardado en sorted_results/TimSort_sorted.csv (tiempo: 12.345ms)
            ‚úÖ QuickSort: guardado en sorted_results/QuickSort_sorted.csv (tiempo: 15.678ms)
            ...
            
            >>> # Personalizar directorio
            >>> analyzer.save_sorted_results(results, "analysis_2025/sorted_data")
        
        Note:
            Los archivos se guardan con encoding UTF-8 sin √≠ndice.
            Solo se guardan algoritmos que ejecutaron exitosamente
            (df is not None).
        """
        import os
        
        # Crear directorio si no existe
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"üíæ Guardando resultados en directorio: {output_dir}")
        
        # Guardar cada resultado
        for name, (df, time_taken) in results.items():
            if df is not None:
                filename = f"{output_dir}/{name}_sorted.csv"
                df.to_csv(filename, index=False, encoding='utf-8')
                print(f"‚úÖ {name}: guardado en {filename} (tiempo: {time_taken*1000:.3f}ms)")

    def generate_complete_report(self, output_base: str = "academic_sorting_analysis"):
        """
        Genera reporte completo con an√°lisis de ordenamiento y autores.
        
        Este es el m√©todo "todo en uno" que ejecuta el an√°lisis completo:
        1. Ejecuta todos los algoritmos
        2. Genera gr√°fico de comparaci√≥n de tiempos
        3. Analiza top 15 autores
        4. Guarda resultados ordenados
        5. Crea reporte de texto con estad√≠sticas
        
        Args:
            output_base (str, optional): Nombre base para archivos de salida.
                Por defecto "academic_sorting_analysis".
        
        Files Generated:
            1. {output_base}_time_comparison.png - Gr√°fico de barras
            2. {output_base}_top_authors.csv - Top autores
            3. {output_base}_sorted_data/ - Directorio con CSVs ordenados
            4. {output_base}_execution_times.txt - Reporte de texto
        
        Example:
            >>> analyzer.generate_complete_report("ml_articles_analysis")
            üìã Generando reporte completo...
            üöÄ Ejecutando todos los algoritmos de ordenamiento...
            ...
            üìä Gr√°fico guardado en: ml_articles_analysis_time_comparison.png
            üìù Top autores guardado en: ml_articles_analysis_top_authors.csv
            üíæ Guardando resultados en directorio: ml_articles_analysis_sorted_data
            üìä Reporte de tiempos guardado en: ml_articles_analysis_execution_times.txt
            üéâ An√°lisis completo finalizado!
        
        Report Contents:
            El archivo .txt incluye:
            - Nombre del archivo analizado
            - Total de registros
            - Lista de algoritmos ordenada por tiempo
            - Tiempo de ejecuci√≥n de cada uno en ms
        
        Use Cases:
            - An√°lisis completo de un dataset
            - Benchmarking de rendimiento
            - Generaci√≥n de informes acad√©micos
            - Comparaci√≥n de datasets
        """
        print("üìã Generando reporte completo...")
        
        # ===== PASO 1: EJECUTAR TODOS LOS ALGORITMOS =====
        results = self.run_all_algorithms()
        
        # ===== PASO 2: CREAR GR√ÅFICO DE TIEMPOS =====
        chart_path = f"{output_base}_time_comparison.png"
        self.create_time_comparison_chart(results, chart_path)
        
        # ===== PASO 3: ANALIZAR TOP AUTORES =====
        top_authors = self.get_top_authors(15)
        authors_path = f"{output_base}_top_authors.csv"
        top_authors.to_csv(authors_path, index=False, encoding='utf-8')
        print(f"üìù Top autores guardado en: {authors_path}")
        
        # ===== PASO 4: GUARDAR RESULTADOS ORDENADOS =====
        self.save_sorted_results(results, f"{output_base}_sorted_data")
        
        # ===== PASO 5: CREAR REPORTE DE TEXTO =====
        times_report = f"{output_base}_execution_times.txt"
        with open(times_report, 'w', encoding='utf-8') as f:
            f.write("=== REPORTE DE TIEMPOS DE EJECUCI√ìN ===\n\n")
            f.write(f"Archivo analizado: {self.csv_file}\n")
            total_registros = len(self.df) if isinstance(self.df, pd.DataFrame) else 0
            f.write(f"Total de registros: {total_registros}\n\n")
            
            # Ordenar resultados por tiempo
            sorted_times = sorted(
                [(name, time_taken) for name, (df, time_taken) in results.items()], 
                key=lambda x: x[1]
            )
            
            f.write("TIEMPOS DE EJECUCI√ìN (ordenado ascendente):\n")
            f.write("-" * 50 + "\n")
            
            for i, (name, time_taken) in enumerate(sorted_times, 1):
                if time_taken != float('inf'):
                    f.write(f"{i:2d}. {name:<20}: {time_taken*1000:8.3f} ms\n")
                else:
                    f.write(f"{i:2d}. {name:<20}: ERROR\n")
        
        print(f"üìä Reporte de tiempos guardado en: {times_report}")
        print("üéâ An√°lisis completo finalizado!")


# ============================================================================
# FUNCI√ìN DE CONVENIENCIA
# ============================================================================

def analyze_academic_data(csv_file: str, output_base: str = "src/data/csv/academic_analysis"):
    """
    Funci√≥n de conveniencia para analizar datos acad√©micos con un solo comando.
    
    Wrapper que simplifica el uso de AcademicSortingAnalyzer ejecutando
    autom√°ticamente el an√°lisis completo y retornando la instancia del
    analizador para an√°lisis adicional si es necesario.
    
    Args:
        csv_file (str): Ruta al archivo CSV con datos acad√©micos
        output_base (str, optional): Nombre base para archivos de salida.
            Por defecto "academic_analysis".
    
    Returns:
        AcademicSortingAnalyzer: Instancia del analizador con datos cargados
            y an√°lisis completado.
    
    Process:
        1. Crea instancia de AcademicSortingAnalyzer
        2. Llama a generate_complete_report()
        3. Retorna analizador para uso adicional
    
    Example:
        >>> # Uso simple - an√°lisis completo
        >>> analyzer = analyze_academic_data("articles.csv")
        
        >>> # Uso con nombre personalizado
        >>> analyzer = analyze_academic_data(
        ...     "ebsco_data.csv",
        ...     output_base="ebsco_analysis_2025"
        ... )
        
        >>> # Continuar con an√°lisis adicional
        >>> top_20_authors = analyzer.get_top_authors(20)
        >>> print(top_20_authors.head())
    
    Note:
        Esta es la forma m√°s r√°pida de obtener un an√°lisis completo.
        Para m√°s control, usar AcademicSortingAnalyzer directamente.
    """
    # Crear instancia del analizador
    analyzer = AcademicSortingAnalyzer(csv_file)
    
    # Ejecutar an√°lisis completo
    analyzer.generate_complete_report(output_base)
    
    return analyzer
