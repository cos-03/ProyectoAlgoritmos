"""
EBSCO Academic Database Scraper
================================

Este m√≥dulo proporciona una clase para realizar web scraping de la base de datos
acad√©mica EBSCO (https://www.ebsco.com/), permitiendo extraer art√≠culos cient√≠ficos,
papers y documentaci√≥n acad√©mica de manera automatizada.

El scraper maneja autom√°ticamente:
- Autenticaci√≥n mediante navegador (Playwright)
- Gesti√≥n de cookies y sesiones
- Extracci√≥n de metadatos de art√≠culos
- Exportaci√≥n a CSV/JSON
- Rate limiting y manejo de errores

Requisitos:
-----------
- requests: Para realizar peticiones HTTP
- playwright: Para automatizar el navegador y manejar login
- pandas: Para manipulaci√≥n de datos (opcional)

Fecha: 2025
"""

import requests
import json
import time
import csv
import pandas as pd
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright
from typing import Any
import os
import random
from pathlib import Path


class EBSCOScraper:
    """
    Scraper para la base de datos acad√©mica EBSCO.
    
    Esta clase proporciona m√©todos para autenticarse en EBSCO mediante login
    institucional, realizar b√∫squedas de art√≠culos acad√©micos y extraer sus
    metadatos completos incluyendo t√≠tulos, autores, abstracts, DOIs, etc.
    
    Attributes:
        base_url (str): URL base de la API de b√∫squeda de EBSCO
        session (requests.Session): Sesi√≥n HTTP para mantener cookies
        login_url (str): URL de inicio de sesi√≥n institucional
        headers (dict): Headers HTTP para las peticiones
        cookies (dict): Cookies de sesi√≥n para autenticaci√≥n
        total_items (int): N√∫mero total de resultados disponibles
    
    Example:
        >>> scraper = EBSCOScraper(auto_login=True)
        >>> articles = scraper.scrape_all("machine learning", max_results=100)
        >>> scraper.save_to_csv(articles, "ml_articles.csv")
    """
    
    def __init__(self, auto_login: bool = True):
        """
        Inicializa el scraper de EBSCO.
        
        Configura la sesi√≥n HTTP, URLs, headers y opcionalmente realiza el
        login autom√°tico. Si auto_login es True, intentar√° cargar cookies
        existentes o iniciar√° un proceso de login manual si es necesario.
        
        Args:
            auto_login (bool, optional): Si es True, intenta autenticarse
                autom√°ticamente al inicializar. Por defecto True.
        
        Raises:
            Exception: Si el auto_login falla y no se puede establecer sesi√≥n
        """
        # URL de la API de b√∫squeda de EBSCO
        self.base_url = (
            "https://research-ebsco-com.crai.referencistas.com/api/search/v1/search"
        )
        
        # Sesi√≥n HTTP para mantener cookies entre peticiones
        self.session = requests.Session()
        
        # URL de acceso institucional con proxy de autenticaci√≥n
        self.login_url = "https://login.intelproxy.com/v2/inicio?cuenta=7Ah6RNpGWF22jjyq&url=ezp.2aHR0cHM6Ly9zZWFyY2guZWJzY29ob3N0LmNvbS9sb2dpbi5hc3B4PyZkaXJlY3Q9dHJ1ZSZzaXRlPWVkcy1saXZlJmF1dGh0eXBlPWlwJmN1c3RpZD1uczAwNDM2MyZnZW9jdXN0aWQ9Jmdyb3VwaWQ9bWFpbiZwcm9maWxlPWVkcyZicXVlcnk9Z2VuZXJhdGl2ZSthcnRpZmljaWFsK2ludGVsbGlnZW5jZQ--"
        
        # Headers HTTP que simulan un navegador real
        self.headers = {
            "Accept": "application/json, text/plain, */*",
            "Content-Type": "application/json",
            "Origin": "https://research-ebsco-com.crai.referencistas.com",
            "Referer": "https://research-ebsco-com.crai.referencistas.com/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "accept-language": "es;q=0.9, es-419;q=0.8, es;q=0.7",
            "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "txn-route": "true",
            "x-eis-gateway-referrer-from-ui": "same-site",
            "x-initiated-by": "refresh",
        }

        # Diccionario para almacenar cookies de sesi√≥n
        self.cookies = {}
        
        # Variable para almacenar el total de resultados disponibles
        self.total_items = None

        # Rutas centralizadas para datos y perfil de navegador
        # DATA_ROOT -> <repo>/src/data ; BROWSER_PROFILE_ROOT -> <repo>/browser_profile
        self.DATA_ROOT = Path(__file__).parents[1] / "data"
        self.BROWSER_PROFILE_ROOT = Path(__file__).parents[2] / "browser_profile"

        # Proceso de autenticaci√≥n autom√°tica
        if auto_login:
            # Intentar cargar cookies existentes primero
            if not (self.load_cookies() and self.test_cookies()):
                print("Cookies no v√°lidas o no encontradas. Iniciando login manual...")
                self.manual_login()

    def manual_login(self):
        """
        Realiza el proceso de login completamente manual.
        
        Abre un navegador Chromium donde el usuario debe completar manualmente
        el proceso de autenticaci√≥n institucional. Una vez completado, extrae
        las cookies de sesi√≥n y las guarda para futuros usos.
        
        Este m√©todo es √∫til cuando:
        - Las cookies han expirado
        - El login autom√°tico falla
        - Se requiere autenticaci√≥n de dos factores
        - Es la primera vez que se usa el scraper
        
        Process:
            1. Abre navegador Chromium (headless=False)
            2. Navega a la URL de login institucional
            3. Espera a que el usuario complete el login
            4. Extrae cookies del contexto del navegador
            5. Guarda cookies en archivo JSON
        
        Raises:
            Exception: Si hay errores durante el proceso de navegaci√≥n o
                extracci√≥n de cookies
        
        Note:
            El navegador permanecer√° abierto hasta que el usuario presione
            Enter en la consola, indicando que el login est√° completo.
        """
        print("=== LOGIN MANUAL REQUERIDO ===")
        print("Se abrir√° un navegador. Por favor:")
        print("1. Completa el login manualmente")
        print("2. Navega hasta la p√°gina principal de EBSCO")
        print("3. Presiona Enter en esta consola cuando est√©s listo")
        
        # Iniciar Playwright para automatizar el navegador
        with sync_playwright() as p:
            # Lanzar navegador Chromium visible (headless=False)
            browser = p.chromium.launch(headless=False)
            
            # Crear contexto con user agent personalizado
            context = browser.new_context(
                user_agent=self.headers["User-Agent"]
            )
            page = context.new_page()

            try:
                # Navegar a la p√°gina de login institucional
                page.goto(self.login_url)
                
                # Esperar confirmaci√≥n del usuario
                print("\nPor favor completa el login en el navegador...")
                print("Presiona Enter cuando hayas terminado y est√©s en EBSCO:")
                input()
                
                # Verificar que estamos en la p√°gina correcta de EBSCO
                current_url = page.url
                if "ebsco" not in current_url.lower() and "crai.referencistas" not in current_url:
                    print("Navegando a EBSCO...")
                    ebsco_url = "https://research-ebsco-com.crai.referencistas.com/"
                    page.goto(ebsco_url)
                    page.wait_for_timeout(3000)
                
                # Extraer todas las cookies del contexto del navegador
                cookies = context.cookies()
                safe_cookies: Dict[str, str] = {}
                for c in cookies:
                    name = c.get("name")
                    value = c.get("value")
                    if name and value:
                        safe_cookies[name] = value
                
                # Almacenar cookies en la instancia
                self.cookies = safe_cookies
                print(f"Cookies extra√≠das: {len(self.cookies)} cookies")
                
                # Guardar cookies en archivo para uso futuro
                self.save_cookies()
                
                print("‚úì Login completado exitosamente")

            except Exception as e:
                print(f"Error durante el login manual: {e}")
                raise
            finally:
                # Cerrar navegador siempre, incluso si hay errores
                browser.close()

    def login_with_persistent_browser(self):
        """
        Utiliza un perfil de navegador persistente para mantener la sesi√≥n.
        
        Este m√©todo crea y utiliza un directorio de perfil de usuario para
        Chromium, lo que permite que las cookies y la sesi√≥n persistan entre
        ejecuciones. Es √∫til para evitar tener que hacer login repetidamente.
        
        Features:
            - Guarda el estado del navegador en disco
            - Mantiene cookies entre sesiones
            - Evita repetir el proceso de login
            - √ötil para desarrollo y pruebas
        
        Process:
            1. Crea directorio './browser_profile' si no existe
            2. Lanza navegador con perfil persistente
            3. Usuario completa login (solo primera vez)
            4. Extrae y guarda cookies
            5. Sesiones futuras reutilizan el perfil
        
        Note:
            El perfil del navegador puede crecer en tama√±o con el tiempo.
            Se recomienda limpiarlo peri√≥dicamente.
        
        Warning:
            No compartir el directorio browser_profile ya que contiene
            datos sensibles de sesi√≥n.
        """
        print("=== LOGIN CON PERFIL PERSISTENTE ===")
        
        # Crear directorio para almacenar el perfil del navegador en ruta unificada
        profile_dir = self.BROWSER_PROFILE_ROOT / "ebsco"
        profile_dir.mkdir(parents=True, exist_ok=True)
        
        with sync_playwright() as p:
            # Lanzar navegador con contexto persistente
            # Esto guarda cookies, localStorage, etc. en disco
            browser = p.chromium.launch_persistent_context(
                user_data_dir=str(profile_dir),
                headless=False,
                user_agent=self.headers["User-Agent"]
            )
            
            try:
                page = browser.new_page()
                page.goto(self.login_url)
                
                print("Completa el login en el navegador...")
                print("El navegador guardar√° tu sesi√≥n para futuros usos.")
                print("Presiona Enter cuando hayas completado el login:")
                input()
                
                # Navegar a EBSCO si no estamos ah√≠ ya
                if "ebsco" not in page.url.lower():
                    page.goto("https://research-ebsco-com.crai.referencistas.com/c/rfbjy2/search")
                    page.wait_for_timeout(3000)
                
                # Extraer cookies del contexto persistente
                cookies = browser.cookies()
                safe_cookies: Dict[str, str] = {}
                for c in cookies:
                    name = c.get("name")
                    value = c.get("value")
                    if name and value:
                        safe_cookies[name] = value
                
                self.cookies = safe_cookies
                self.save_cookies()
                
                print("‚úì Login con perfil persistente completado")

            except Exception as e:
                print(f"Error con perfil persistente: {e}")
                raise
            finally:
                browser.close()

    def login_and_get_cookies(self, email: Optional[str] = None, password: Optional[str] = None, headless: bool = False):
        """
        M√©todo avanzado de login con automatizaci√≥n completa y fallback manual.
        
        Este m√©todo intenta automatizar completamente el proceso de login mediante
        Google SSO. Si la automatizaci√≥n falla, hace fallback a login manual.
        Incluye detecci√≥n anti-bot y manejo de m√∫ltiples escenarios de login.
        
        Args:
            email (Optional[str]): Email de Google para login autom√°tico.
                Si no se proporciona, el usuario debe ingresar manualmente.
            password (Optional[str]): Contrase√±a de Google para login autom√°tico.
                Si no se proporciona, el usuario debe ingresar manualmente.
            headless (bool, optional): Si es True, ejecuta el navegador en modo
                headless (sin interfaz gr√°fica). Por defecto False.
        
        Features:
            - Automatizaci√≥n completa del flujo de login de Google
            - Detecci√≥n y clic en bot√≥n de Google SSO
            - Ingreso autom√°tico de credenciales
            - Anti-detecci√≥n de bots (oculta webdriver)
            - Screenshots para debugging
            - Fallback a modo manual si falla automatizaci√≥n
            - M√∫ltiples selectores para mayor compatibilidad
        
        Process:
            1. Configura navegador anti-detecci√≥n
            2. Navega a p√°gina de login
            3. Detecta y hace clic en bot√≥n de Google
            4. Ingresa email y password si est√°n disponibles
            5. Espera redirecci√≥n a EBSCO
            6. Extrae y guarda cookies
        
        Raises:
            Exception: Si el proceso falla completamente y no es posible
                establecer sesi√≥n ni manual ni autom√°ticamente
        
        Note:
            El modo headless puede ser detectado por algunos sistemas anti-bot.
            Se recomienda usar headless=False para mayor confiabilidad.
        
        Example:
            >>> scraper = EBSCOScraper(auto_login=False)
            >>> scraper.login_and_get_cookies(
            ...     email="usuario@universidad.edu",
            ...     password="contrase√±a_segura",
            ...     headless=False
            ... )
        """
        print("Iniciando proceso de autenticaci√≥n...")
        
        with sync_playwright() as p:
            # Configurar navegador con argumentos anti-detecci√≥n
            browser = p.chromium.launch(
                headless=headless,
                args=[
                    '--disable-blink-features=AutomationControlled',  # Oculta que es automatizado
                    '--disable-dev-shm-usage',  # Mejora rendimiento en Linux
                    '--no-sandbox',  # Necesario en algunos entornos
                    '--disable-extensions',  # Desactiva extensiones
                    '--disable-plugins-discovery',
                    '--disable-web-security',  # Solo para testing
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            # Crear contexto con configuraci√≥n realista
            context = browser.new_context(
                user_agent=self.headers["User-Agent"],
                viewport={'width': 1920, 'height': 1080},  # Resoluci√≥n com√∫n
                extra_http_headers={
                    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
                }
            )
            
            # Inyectar script para ocultar propiedades de automatizaci√≥n
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                window.chrome = {
                    runtime: {}
                };
            """)
            
            page = context.new_page()

            try:
                print("Navegando a la p√°gina de login...")
                # Esperar a que la red est√© inactiva (p√°gina completamente cargada)
                page.goto(self.login_url, wait_until='networkidle')
                
                # Esperar tiempo adicional para JavaScript din√°mico
                page.wait_for_timeout(5000)
                
                print("Buscando bot√≥n de Google...")
                
                # Tomar screenshot para debugging (guardar en carpeta organizada dentro de src/data)
                screenshots_dir = self.DATA_ROOT / "screenshots"
                screenshots_dir.mkdir(parents=True, exist_ok=True)
                screenshot_path = screenshots_dir / "login_page_debug.png"
                page.screenshot(path=str(screenshot_path))
                print(f"Screenshot guardado como '{screenshot_path}'")
                
                # Lista exhaustiva de selectores para encontrar el bot√≥n de Google
                google_selectors = [
                    'button:has-text("Google")',
                    'a:has-text("Google")',
                    'button:has-text("Gmail")',
                    'a:has-text("Gmail")',
                    '[data-provider="google"]',
                    '.google-login',
                    '#google-login',
                    'button[title*="Google"]',
                    'a[href*="google"]',
                    'button[class*="google"]',
                    'a[class*="google"]',
                    'button:has([class*="google"])',
                    'a:has([class*="google"])',
                    'div[role="button"]:has-text("Google")',
                ]
                
                # Intentar encontrar bot√≥n de Google con m√∫ltiples selectores
                google_button = None
                for selector in google_selectors:
                    try:
                        element = page.wait_for_selector(selector, timeout=3000)
                        if element and element.is_visible():
                            print(f"‚úì Bot√≥n de Google encontrado: {selector}")
                            google_button = element
                            break
                    except:
                        continue
                
                if not google_button:
                    # No se encontr√≥ bot√≥n de Google
                    print("‚ùå No se encontr√≥ bot√≥n de Google")
                    if not headless:
                        print("Cambiando a modo manual...")
                        input("Por favor, realiza el login manualmente y presiona Enter...")
                    else:
                        # Si estamos en headless, reintentar en modo visible
                        browser.close()
                        return self.manual_login()
                else:
                    # ===== LOGIN AUTOM√ÅTICO DE GOOGLE =====
                    print("üöÄ Iniciando login autom√°tico...")
                    
                    # Hacer scroll al bot√≥n si es necesario
                    google_button.scroll_into_view_if_needed()
                    page.wait_for_timeout(1000)
                    google_button.click()
                    print("‚úì Click en bot√≥n de Google")
                    
                    # Esperar redirecci√≥n a Google
                    page.wait_for_timeout(3000)
                    
                    # Verificar que estamos en la p√°gina de login de Google
                    if "google" in page.url.lower() or "accounts.google.com" in page.url:
                        print("‚úì Redirigido a Google")
                        
                        if email and password:
                            # === AUTOMATIZAR LOGIN COMPLETO ===
                            print("üîë Automatizando login con credenciales...")
                            
                            try:
                                # ===== PASO 1: INGRESAR EMAIL =====
                                print("Ingresando email...")
                                email_selectors = [
                                    'input[type="email"]',
                                    'input[name="identifier"]',
                                    'input[id="identifierId"]',
                                    '#Email',
                                    'input[aria-label*="email"]',
                                    'input[aria-label*="correo"]'
                                ]
                                
                                email_input = None
                                for selector in email_selectors:
                                    try:
                                        email_input = page.wait_for_selector(selector, timeout=5000)
                                        if email_input and email_input.is_visible():
                                            print(f"‚úì Campo email encontrado: {selector}")
                                            break
                                    except:
                                        continue
                                
                                if email_input:
                                    # Limpiar campo y escribir email
                                    email_input.click()
                                    page.keyboard.press("Control+a")
                                    email_input.fill(email)
                                    page.wait_for_timeout(1000)
                                    
                                    # Buscar bot√≥n "Siguiente"
                                    next_selectors = [
                                        'button:has-text("Next")',
                                        'button:has-text("Siguiente")',
                                        'input[type="submit"]',
                                        '#identifierNext',
                                        'button[id*="next"]',
                                        'button[class*="next"]'
                                    ]
                                    
                                    next_button = None
                                    for selector in next_selectors:
                                        try:
                                            next_button = page.wait_for_selector(selector, timeout=3000)
                                            if next_button and next_button.is_visible():
                                                print(f"‚úì Bot√≥n siguiente encontrado: {selector}")
                                                break
                                        except:
                                            continue
                                    
                                    if next_button:
                                        next_button.click()
                                        print("‚úì Email enviado")
                                    else:
                                        # Fallback: presionar Enter
                                        page.keyboard.press("Enter")
                                        print("‚úì Enter presionado para email")
                                    
                                    page.wait_for_timeout(3000)
                                else:
                                    raise Exception("No se encontr√≥ campo de email")
                                
                                # ===== PASO 2: INGRESAR CONTRASE√ëA =====
                                print("Esperando campo de contrase√±a...")
                                password_selectors = [
                                    'input[type="password"]',
                                    'input[name="password"]',
                                    'input[aria-label*="password"]',
                                    'input[aria-label*="contrase√±a"]',
                                    '#password',
                                    'input[name="Passwd"]'
                                ]
                                
                                password_input = None
                                for selector in password_selectors:
                                    try:
                                        password_input = page.wait_for_selector(selector, timeout=10000)
                                        if password_input and password_input.is_visible():
                                            print(f"‚úì Campo contrase√±a encontrado: {selector}")
                                            break
                                    except:
                                        continue
                                
                                if password_input:
                                    # Escribir contrase√±a
                                    password_input.click()
                                    password_input.fill(password)
                                    page.wait_for_timeout(1000)
                                    
                                    # Buscar bot√≥n para enviar contrase√±a
                                    login_selectors = [
                                        'button:has-text("Next")',
                                        'button:has-text("Siguiente")',
                                        'button:has-text("Sign in")',
                                        'button:has-text("Iniciar sesi√≥n")',
                                        'input[type="submit"]',
                                        '#passwordNext',
                                        'button[id*="next"]'
                                    ]
                                    
                                    login_button = None
                                    for selector in login_selectors:
                                        try:
                                            login_button = page.wait_for_selector(selector, timeout=3000)
                                            if login_button and login_button.is_visible():
                                                print(f"‚úì Bot√≥n login encontrado: {selector}")
                                                break
                                        except:
                                            continue
                                    
                                    if login_button:
                                        login_button.click()
                                        print("‚úì Contrase√±a enviada")
                                    else:
                                        page.keyboard.press("Enter")
                                        print("‚úì Enter presionado para contrase√±a")
                                    
                                    print("‚è≥ Esperando completar autenticaci√≥n...")
                                    page.wait_for_timeout(5000)
                                    
                                else:
                                    raise Exception("No se encontr√≥ campo de contrase√±a")
                                
                            except Exception as e:
                                print(f"‚ùå Error en login autom√°tico: {e}")
                                if not headless:
                                    print("üîÑ Cambiando a modo manual...")
                                    input("Completa el login manualmente y presiona Enter...")
                                else:
                                    raise
                        else:
                            # Sin credenciales - modo manual
                            print("üìù Sin credenciales - completar manualmente...")
                            if not headless:
                                input("Por favor completa el login de Google y presiona Enter...")
                            else:
                                browser.close()
                                return self.login_and_get_cookies(email, password, headless=False)
                    else:
                        print("‚ùå No se redirigi√≥ a Google correctamente")
                        if not headless:
                            input("Por favor completa el login manualmente y presiona Enter...")
                
                # ===== VERIFICAR LLEGADA A EBSCO =====
                print("üîç Esperando llegada a EBSCO...")
                for i in range(30):  # 30 segundos m√°ximo
                    current_url = page.url
                    if "ebsco" in current_url.lower() or "crai.referencistas" in current_url:
                        print(f"‚úÖ Llegamos a EBSCO: {current_url}")
                        break
                    page.wait_for_timeout(1000)
                    if i % 5 == 0:
                        print(f"‚è≥ Esperando... URL actual: {current_url[:100]}...")
                else:
                    # Si no llegamos autom√°ticamente, navegar manualmente
                    print("‚ö†Ô∏è No llegamos a EBSCO autom√°ticamente, intentando navegar...")
                    try:
                        page.goto("https://research-ebsco-com.crai.referencistas.com/")
                        page.wait_for_timeout(3000)
                    except:
                        pass
                
                # ===== EXTRAER COOKIES =====
                cookies = context.cookies()
                safe_cookies: Dict[str, str] = {}
                for c in cookies:
                    name = c.get("name")
                    value = c.get("value")
                    if name and value:
                        safe_cookies[name] = value
                
                self.cookies = safe_cookies
                print(f"üç™ {len(self.cookies)} cookies extra√≠das")
                
                # Guardar cookies para uso futuro
                self.save_cookies()
                
                print("üéâ Login completado exitosamente!")

            except Exception as e:
                print(f"‚ùå Error durante el login: {e}")
                if not headless:
                    print("üîÑ Fallback a modo manual...")
                    input("Por favor completa el login manualmente y presiona Enter...")
                    
                    # Extraer cookies despu√©s del login manual
                    try:
                        cookies = context.cookies()
                        safe_cookies: Dict[str, str] = {}
                        for c in cookies:
                            name = c.get("name")
                            value = c.get("value")
                            if name and value:
                                safe_cookies[name] = value
                        
                        self.cookies = safe_cookies
                        self.save_cookies()
                        print(f"üç™ {len(self.cookies)} cookies guardadas desde login manual")
                    except:
                        pass
                else:
                    raise
            finally:
                browser.close()

    def save_cookies(self, filename: str = "ebsco_cookies.json"):
        """
        Guarda las cookies de sesi√≥n en un archivo JSON.
        
        Serializa el diccionario de cookies a formato JSON y lo guarda en disco
        para poder reutilizar la sesi√≥n en ejecuciones futuras sin necesidad
        de volver a hacer login.
        
        Args:
            filename (str, optional): Nombre del archivo donde guardar las cookies.
                Por defecto "ebsco_cookies.json".
        
        Note:
            Las cookies contienen tokens de sesi√≥n sensibles. No compartir
            ni subir a repositorios p√∫blicos.
        
        Security:
            Se recomienda agregar *.json al .gitignore para evitar exponer
            las cookies en control de versiones.
        
        Example:
            >>> scraper.save_cookies("mi_sesion.json")
            Cookies guardadas en: mi_sesion.json
        """
        # Resolver ruta objetivo: si solo es nombre, usar src/data/cookies/<filename>
        if not os.path.dirname(filename):
            cookies_dir = self.DATA_ROOT / "cookies"
            cookies_dir.mkdir(parents=True, exist_ok=True)
            fullpath = cookies_dir / filename
        else:
            fullpath = Path(filename)
            if fullpath.parent:
                fullpath.parent.mkdir(parents=True, exist_ok=True)

        with open(str(fullpath), 'w', encoding='utf-8') as f:
            json.dump(self.cookies, f, indent=2)
        print(f"Cookies guardadas en: {fullpath}")

    def load_cookies(self, filename: str = "ebsco_cookies.json") -> bool:
        """
        Carga cookies de sesi√≥n desde un archivo JSON.
        
        Intenta cargar cookies previamente guardadas desde un archivo. Si el
        archivo existe y se carga correctamente, retorna True. Si no existe
        o hay alg√∫n error, retorna False.
        
        Args:
            filename (str, optional): Nombre del archivo de cookies a cargar.
                Por defecto "ebsco_cookies.json".
        
        Returns:
            bool: True si las cookies se cargaron exitosamente, False en caso contrario.
        
        Raises:
            No lanza excepciones - captura errores y retorna False.
        
        Example:
            >>> scraper = EBSCOScraper(auto_login=False)
            >>> if scraper.load_cookies():
            ...     print("Cookies cargadas, sesi√≥n restaurada")
            ... else:
            ...     print("Cookies no disponibles, hacer login")
        """
        try:
            # Resolver ruta desde src/data/cookies si solo es nombre
            if not os.path.dirname(filename):
                fullpath = self.DATA_ROOT / "cookies" / filename
            else:
                fullpath = Path(filename)

            if os.path.exists(str(fullpath)):
                with open(str(fullpath), 'r', encoding='utf-8') as f:
                    self.cookies = json.load(f)
                print(f"Cookies cargadas desde: {fullpath}")
                return True
            else:
                print(f"Archivo de cookies no encontrado: {fullpath}")
                return False
        except Exception as e:
            print(f"Error cargando cookies: {e}")
            return False

    def test_cookies(self) -> bool:
        """
        Verifica si las cookies actuales son v√°lidas.
        
        Realiza una b√∫squeda de prueba con 1 resultado para verificar que
        las cookies de sesi√≥n siguen siendo v√°lidas y permiten acceso a la API.
        
        Returns:
            bool: True si las cookies son v√°lidas y permiten b√∫squedas,
                  False si las cookies est√°n expiradas o son inv√°lidas.
        
        Note:
            Este m√©todo hace una petici√≥n real a la API, por lo que consume
            una llamada de tu cuota si existe l√≠mite de rate.
        
        Example:
            >>> if not scraper.test_cookies():
            ...     print("Cookies expiradas, re-autenticando...")
            ...     scraper.manual_login()
        """
        try:
            # Hacer una b√∫squeda m√≠nima de prueba
            test_data = self.search("artificial intelligence", offset=0, count=1, verbose=False)
            is_valid = test_data.get('totalItems', 0) >= 0
            if is_valid:
                print("‚úì Cookies v√°lidas")
            else:
                print("‚úó Cookies inv√°lidas")
            return is_valid
        except Exception as e:
            print(f"‚úó Cookies no v√°lidas: {e}")
            return False

    def get_total_items(self, query: str) -> int:
        """
        Obtiene el n√∫mero total de resultados disponibles para una b√∫squeda.
        
        Realiza una b√∫squeda solicitando solo 1 resultado para obtener el
        contador total de items disponibles sin consumir ancho de banda
        innecesariamente.
        
        Args:
            query (str): T√©rmino o t√©rminos de b√∫squeda.
        
        Returns:
            int: N√∫mero total de resultados disponibles para la b√∫squeda.
                 Retorna 0 si hay error o no hay resultados.
        
        Note:
            Este n√∫mero representa el total de documentos que coinciden con
            la b√∫squeda, no cu√°ntos puedes descargar (puede haber l√≠mites).
        
        Raises:
            No lanza excepciones - captura errores y retorna 0.
        
        Example:
            >>> total = scraper.get_total_items("machine learning")
            Total de resultados disponibles para 'machine learning': 45,321
            >>> print(f"Hay {total} art√≠culos disponibles")
        """
        payload = self._build_payload(query, offset=0, count=1)

        try:
            response = self.session.post(
                self.base_url,
                headers=self.headers,
                cookies=self.cookies,
                json=payload,
                params={
                    "applyAllLimiters": "true",
                    "includeSavedItems": "false",
                    "excludeLinkValidation": "true",
                },
            )
            response.raise_for_status()
            
            data = response.json()
            total = data.get("search", {}).get("totalItems", 0)
            print(f"Total de resultados disponibles para '{query}': {total:,}")
            return total

        except Exception as e:
            print(f"Error obteniendo total de items: {e}")
            if "401" in str(e) or "403" in str(e):
                print("Posible problema de autenticaci√≥n. Cookies pueden haber expirado.")
            return 0

    def _build_payload(self, query: str, offset: int = 0, count: int = 50) -> Dict:
        """
        Construye el payload JSON para las peticiones a la API de EBSCO.
        
        M√©todo privado que genera la estructura JSON necesaria para realizar
        b√∫squedas en la API de EBSCO, incluyendo filtros, expansores y
        par√°metros de paginaci√≥n.
        
        Args:
            query (str): T√©rmino de b√∫squeda.
            offset (int, optional): Posici√≥n de inicio para paginaci√≥n (0-indexed).
                Por defecto 0.
            count (int, optional): N√∫mero de resultados a retornar por p√°gina.
                Por defecto 50.
        
        Returns:
            Dict: Diccionario con la estructura completa del payload para la API.
        
        Payload Structure:
            - advancedSearchStrategy: Tipo de estrategia de b√∫squeda
            - query: T√©rmino de b√∫squeda
            - autoCorrect: Si se debe auto-corregir ortograf√≠a
            - profileIdentifier: ID del perfil institucional
            - expanders: Lista de expansores (tesauro, conceptos)
            - filters: Filtros aplicados (texto completo, etc.)
            - searchMode: Modo de b√∫squeda ("all" busca todas las palabras)
            - sort: Orden de resultados (relevancia, fecha, etc.)
            - offset: Posici√≥n inicial para paginaci√≥n
            - count: N√∫mero de resultados por p√°gina
            - highlightTag: Tag HTML para resaltar coincidencias
        
        Note:
            Este m√©todo es privado (prefijo _) y normalmente no debe ser
            llamado directamente por usuarios de la clase.
        """
        return {
            "advancedSearchStrategy": "NONE",  # B√∫squeda simple (no avanzada)
            "query": query,  # T√©rmino de b√∫squeda
            "autoCorrect": False,  # No corregir autom√°ticamente errores
            "profileIdentifier": "q46rpe",  # ID del perfil institucional
            "expanders": ["thesaurus", "concept"],  # Expandir con sin√≥nimos y conceptos
            "filters": [
                {"id": "FT", "values": ["true"]},  # Solo texto completo (Full Text)
                {"id": "FT1", "values": ["true"]},  # Texto completo disponible
            ],
            "searchMode": "all",  # Buscar TODAS las palabras (AND)
            "sort": "relevance",  # Ordenar por relevancia
            "isNovelistEnabled": False,  # No incluir contenido de Novelist
            "includePlacards": True,  # Incluir anuncios/destacados
            "offset": offset,  # Posici√≥n inicial (paginaci√≥n)
            "count": count,  # N√∫mero de resultados a retornar
            "highlightTag": "mark",  # Tag HTML para resaltar coincidencias
            "userDirectAction": False,  # No es acci√≥n directa del usuario
        }

    def search(self, query: str, offset: int = 0, count: int = 50, verbose: bool = True) -> Dict:
        """
        Realiza una b√∫squeda en la base de datos EBSCO.
        
        Ejecuta una petici√≥n de b√∫squeda a la API de EBSCO y retorna los
        resultados en formato JSON. Incluye rate limiting autom√°tico para
        evitar bloqueos por exceso de peticiones.
        
        Args:
            query (str): T√©rmino o t√©rminos de b√∫squeda. Puede incluir
                operadores booleanos (AND, OR, NOT) y comillas para frases.
            offset (int, optional): Posici√≥n de inicio para paginaci√≥n (0-indexed).
                Por defecto 0.
            count (int, optional): N√∫mero de resultados a retornar (1-50).
                Por defecto 50.
            verbose (bool, optional): Si es True, imprime informaci√≥n de debug.
                Por defecto True.
        
        Returns:
            Dict: Respuesta JSON de la API con los resultados de b√∫squeda.
        
        Raises:
            requests.exceptions.HTTPError: Si la petici√≥n falla (401, 403, 500, etc.)
            requests.exceptions.RequestException: Para errores de red
        
        Response Structure:
            {
                "search": {
                    "totalItems": int,  # Total de resultados
                    "items": [...]      # Lista de art√≠culos
                }
            }
        
        Example:
            >>> results = scraper.search("artificial intelligence", offset=0, count=10)
            üì° Query buscado: 'artificial intelligence'
            üì° Status code: 200
            >>> print(f"Encontrados: {results['search']['totalItems']} art√≠culos")
        """
        # Construir payload con par√°metros de b√∫squeda
        payload = self._build_payload(query, offset, count)
        
        # Rate limiting: peque√±o delay para evitar bloqueos
        time.sleep(0.1)

        # Realizar petici√≥n POST a la API
        response = self.session.post(
            self.base_url,
            headers=self.headers,
            cookies=self.cookies,
            json=payload,
            params={
                "applyAllLimiters": "true",  # Aplicar todos los filtros
                "includeSavedItems": "false",  # No incluir items guardados
                "excludeLinkValidation": "true",  # Excluir validaci√≥n de enlaces
            },
        )

        if verbose:
            print(f"üì° Query buscado: '{query}'")
            print(f"üì° Status code: {response.status_code}")

        # Lanzar excepci√≥n si hay error HTTP
        response.raise_for_status()
        return response.json()

    def extract_articles(self, data: Dict) -> List[Dict]:
        """
        Extrae y procesa metadatos de art√≠culos desde la respuesta JSON de la API.
        
        Parsea la respuesta JSON de EBSCO y extrae informaci√≥n estructurada
        de cada art√≠culo, incluyendo t√≠tulo, autores, abstract, DOI, enlaces
        PDF, temas, fechas, y m√°s metadatos bibliogr√°ficos.
        
        Args:
            data (Dict): Respuesta JSON de la API de EBSCO obtenida mediante
                el m√©todo search().
        
        Returns:
            List[Dict]: Lista de diccionarios, donde cada diccionario contiene
                los metadatos completos de un art√≠culo.
        
        Article Structure:
            {
                'id': str,                    # ID √∫nico del art√≠culo
                'title': str,                 # T√≠tulo del art√≠culo
                'abstract': str,              # Resumen/abstract
                'authors': str,               # Autores (separados por ;)
                'publication_date': str,      # Fecha de publicaci√≥n
                'journal': str,               # Nombre de la revista
                'doi': str,                   # Digital Object Identifier
                'subjects': str,              # Temas (separados por ;)
                'page_start': str,            # P√°gina inicial
                'page_end': str,              # P√°gina final
                'volume': str,                # Volumen de la revista
                'issue': str,                 # N√∫mero de la revista
                'publisher': str,             # Editorial
                'pdf_links': str,             # Enlaces PDF (separados por ;)
                'database': str,              # Base de datos de origen
                'peer_reviewed': bool,        # Si est√° revisado por pares
                'language': str,              # Idioma del documento
                'document_type': str,         # Tipo de documento
                'isbn': str,                  # ISBN (para libros)
                'issn': str,                  # ISSN (para revistas)
            }
        
        Note:
            - Los campos m√∫ltiples (autores, temas, PDFs) se unen con ";" 
            - Las etiquetas <mark> de resaltado se eliminan autom√°ticamente
            - Los campos faltantes se rellenan con string vac√≠o ""
        
        Example:
            >>> response = scraper.search("quantum computing", count=5)
            >>> articles = scraper.extract_articles(response)
            üìÑ Extrayendo 5 art√≠culos...
            ‚úÖ 5 art√≠culos extra√≠dos exitosamente
        """
        articles = []
        # Obtener lista de items de la respuesta JSON
        items = data.get("search", {}).get("items", [])
        
        print(f"üìÑ Extrayendo {len(items)} art√≠culos...")
        
        for item in items:
            # Extraer y limpiar t√≠tulo
            title = item.get("title", {}).get("value", "")
            title = title.replace("<mark>", "").replace("</mark>", "")

            # Extraer y limpiar abstract
            abstract = item.get("abstract", {}).get("value", "")
            abstract = abstract.replace("<mark>", "").replace("</mark>", "")

            # Extraer enlaces a PDF
            pdf_links = []
            full_text_links = item.get("links", {}).get("fullTextLinks", [])
            for link in full_text_links:
                if link.get("type") == "pdfFullText":
                    pdf_links.append(link.get("url"))

            # Procesar lista de autores
            authors = []
            for contrib in item.get("contributors", []):
                author_name = contrib.get("name", "")
                if author_name:
                    authors.append(author_name)

            # Procesar lista de temas/keywords
            subjects = []
            for subj in item.get("subjects", []):
                subject_name = subj.get("name", {}).get("value", "")
                if subject_name:
                    subjects.append(subject_name)

            # Construir diccionario con todos los metadatos
            article = {
                "id": item.get("id", ""),
                "title": title,
                "abstract": abstract,
                "authors": "; ".join(authors),  # Unir lista con punto y coma
                "publication_date": item.get("publicationDate", ""),
                "journal": item.get("source", ""),
                "doi": item.get("doi", ""),
                "subjects": "; ".join(subjects),
                "page_start": item.get("pageStart", ""),
                "page_end": item.get("pageEnd", ""),
                "volume": item.get("volume", ""),
                "issue": item.get("issue", ""),
                "publisher": item.get("publisherName", ""),
                "pdf_links": "; ".join(pdf_links),
                "database": item.get("longDBName", ""),
                "peer_reviewed": item.get("peerReviewed", False),
                "language": item.get("language", ""),
                "document_type": item.get("documentType", ""),
                "isbn": item.get("isbn", ""),
                "issn": item.get("issn", ""),
            }
            articles.append(article)
            
        print(f"‚úÖ {len(articles)} art√≠culos extra√≠dos exitosamente")
        return articles

    def scrape_all(
        self,
        query: str,
        max_results: Optional[int] = None,
        batch_size: int = 50,
        delay: float = 0.0,
    ) -> List[Dict]:
        """
        Realiza scraping completo de m√∫ltiples p√°ginas de resultados.
        
        Este es el m√©todo principal para extraer grandes cantidades de art√≠culos.
        Itera sobre todas las p√°ginas de resultados, manejando paginaci√≥n,
        rate limiting, errores de red y re-autenticaci√≥n autom√°tica si es necesario.
        
        Args:
            query (str): T√©rmino de b√∫squeda. Puede incluir operadores booleanos
                (AND, OR, NOT) y comillas para b√∫squeda de frases exactas.
            max_results (Optional[int], optional): N√∫mero m√°ximo de resultados
                a obtener. Si es None, obtiene todos los disponibles. Por defecto None.
            batch_size (int, optional): N√∫mero de resultados por petici√≥n (1-50).
                Valores m√°s altos son m√°s eficientes pero pueden causar timeouts.
                Por defecto 50.
            delay (float, optional): Segundos de espera entre peticiones.
                Se agrega variaci√≥n aleatoria para parecer m√°s humano.
                Por defecto 0.0.
        
        Returns:
            List[Dict]: Lista de todos los art√≠culos extra√≠dos con sus metadatos
                completos. Ver extract_articles() para estructura de cada art√≠culo.
        
        Features:
            - Paginaci√≥n autom√°tica
            - Verificaci√≥n de cookies antes de empezar
            - Re-autenticaci√≥n autom√°tica si las cookies expiran
            - Rate limiting inteligente con variaci√≥n aleatoria
            - Manejo robusto de errores de red
            - Reintentos autom√°ticos con backoff exponencial
            - L√≠mite de errores consecutivos para evitar loops infinitos
            - Progress tracking detallado
        
        Error Handling:
            - M√°ximo 3 errores consecutivos antes de abortar
            - Re-autenticaci√≥n autom√°tica en errores 401/403
            - Backoff exponencial: 5 seg, 10 seg, 15 seg
            - Contin√∫a desde donde se qued√≥ despu√©s de errores
        
        Example:
            >>> # Extraer todos los resultados disponibles
            >>> articles = scraper.scrape_all("climate change")
            
            >>> # Extraer solo los primeros 100 resultados
            >>> articles = scraper.scrape_all(
            ...     query="machine learning",
            ...     max_results=100,
            ...     batch_size=50,
            ...     delay=1.0  # 1 segundo entre peticiones
            ... )
            
            >>> # B√∫squeda con operadores booleanos
            >>> articles = scraper.scrape_all(
            ...     '"artificial intelligence" AND (healthcare OR medicine)',
            ...     max_results=500
            ... )
        
        Progress Output:
            üîç Iniciando scraping para: 'machine learning'
            ‚úì Cookies v√°lidas
            Total de resultados disponibles para 'machine learning': 45,321
            üéØ Objetivo: 100 resultados de 45,321 disponibles
            üì° Scraping offset 0 - 50 (0/100 completado)
            üìÑ Extrayendo 50 art√≠culos...
            ‚úÖ 50 art√≠culos extra√≠dos exitosamente
            ‚è∏Ô∏è Esperando 1.2 segundos...
            üì° Scraping offset 50 - 100 (50/100 completado)
            üéâ Scraping completado: 100 art√≠culos obtenidos
        
        Warning:
            - Respetar rate limits de la instituci√≥n
            - No hacer scraping masivo sin permiso
            - Considerar agregar delay entre peticiones
            - Algunas instituciones limitan el n√∫mero de descargas
        """
        
        print(f"üîç Iniciando scraping para: '{query}'")
        
        # Verificar que las cookies son v√°lidas antes de empezar
        if not self.test_cookies():
            print("Cookies inv√°lidas. Iniciando re-autenticaci√≥n...")
            self.manual_login()

        # Obtener n√∫mero total de resultados disponibles
        total_items = self.get_total_items(query)

        if total_items == 0:
            print("‚ùå No se encontraron resultados para la b√∫squeda")
            return []

        # Determinar cu√°ntos resultados queremos obtener
        target_results = min(max_results or total_items, total_items)
        print(f"üéØ Objetivo: {target_results:,} resultados de {total_items:,} disponibles")

        # Inicializar variables de control
        all_articles = []
        offset = 0
        consecutive_errors = 0
        max_consecutive_errors = 3

        # Loop principal de scraping
        while len(all_articles) < target_results and consecutive_errors < max_consecutive_errors:
            # Calcular cu√°ntos resultados quedan por obtener
            remaining = target_results - len(all_articles)
            current_batch_size = min(batch_size, remaining)

            print(f"üì° Scraping offset {offset:,} - {offset + current_batch_size:,} "
                  f"({len(all_articles):,}/{target_results:,} completado)")

            try:
                # Realizar b√∫squeda para el batch actual
                data = self.search(query, offset, current_batch_size, verbose=True)
                articles = self.extract_articles(data)

                if not articles:
                    print("‚ùå No se encontraron m√°s art√≠culos")
                    break

                # Agregar art√≠culos a la lista completa
                all_articles.extend(articles)
                offset += current_batch_size
                consecutive_errors = 0  # Reset del contador de errores

                # Rate limiting con variaci√≥n aleatoria para parecer humano
                if len(all_articles) < target_results:
                    sleep_time = delay + random.uniform(0, 1)
                    print(f"‚è∏Ô∏è Esperando {sleep_time:.1f} segundos...")
                    time.sleep(sleep_time)

            except requests.exceptions.RequestException as e:
                # Manejo de errores de red
                consecutive_errors += 1
                print(f"‚ùå Error de red ({consecutive_errors}/{max_consecutive_errors}): {e}")
                
                # Verificar si es error de autenticaci√≥n
                if "401" in str(e) or "403" in str(e):
                    print("üîë Error de autenticaci√≥n. Reautenticando...")
                    self.manual_login()
                    consecutive_errors = 0  # Reset despu√©s de reautenticar
                    continue
                
                # Backoff exponencial: esperar m√°s tiempo con cada error
                wait_time = 5 * consecutive_errors
                print(f"‚è≥ Esperando {wait_time} segundos antes de reintentar...")
                time.sleep(wait_time)
                
            except Exception as e:
                # Manejo de errores inesperados
                consecutive_errors += 1
                print(f"‚ùå Error inesperado ({consecutive_errors}/{max_consecutive_errors}): {e}")
                time.sleep(5)

        print(f"üéâ Scraping completado: {len(all_articles):,} art√≠culos obtenidos")
        return all_articles

    def save_to_csv(self, articles: List[Dict], filename: str):
        """
        Guarda los art√≠culos extra√≠dos en un archivo CSV.
        
        Exporta la lista de art√≠culos con todos sus metadatos a formato CSV,
        que puede ser abierto en Excel, Google Sheets, pandas, etc. Maneja
        autom√°ticamente la limpieza de caracteres problem√°ticos y asegura
        compatibilidad con diferentes aplicaciones.
        
        Args:
            articles (List[Dict]): Lista de art√≠culos obtenida de scrape_all()
                o extract_articles().
            filename (str): Ruta y nombre del archivo CSV a crear.
                Si no incluye extensi√≥n .csv, se recomienda agregarla.
        
        Features:
            - Detecta autom√°ticamente todas las columnas presentes
            - Ordena columnas alfab√©ticamente para consistencia
            - Limpia caracteres especiales problem√°ticos
            - Convierte saltos de l√≠nea a espacios
            - Maneja valores None/faltantes
            - Encoding UTF-8 para caracteres internacionales
        
        CSV Structure:
            Las columnas incluir√°n (si est√°n disponibles):
            - id, title, abstract, authors, publication_date
            - journal, doi, subjects, page_start, page_end
            - volume, issue, publisher, pdf_links
            - database, peer_reviewed, language
            - document_type, isbn, issn
        
        Note:
            - Los campos m√∫ltiples est√°n separados por ";"
            - Compatible con Excel y Google Sheets
            - Usar encoding UTF-8 al abrir en Excel para ver acentos
        
        Example:
            >>> articles = scraper.scrape_all("quantum physics", max_results=50)
            >>> scraper.save_to_csv(articles, "quantum_physics_2025.csv")
            üíæ Datos guardados en CSV: quantum_physics_2025.csv
            üìä Total de registros: 50
            üìã Columnas incluidas: 18
            
            >>> # Tambi√©n funciona con rutas completas
            >>> scraper.save_to_csv(articles, "/home/user/data/articles.csv")
        
        Opening in Excel:
            1. Abrir Excel
            2. Data > From Text/CSV
            3. Seleccionar archivo
            4. Asegurar encoding UTF-8
            5. Delimiter: Comma
        
        Reading with pandas:
            >>> import pandas as pd
            >>> df = pd.read_csv("articles.csv", encoding='utf-8')
            >>> print(df.head())
        """
        if not articles:
            print("‚ùå No hay art√≠culos para guardar")
            return
            
        # Obtener todas las columnas √∫nicas de todos los art√≠culos
        all_columns = set()
        for article in articles:
            all_columns.update(article.keys())
        
        # Ordenar columnas alfab√©ticamente para consistencia
        ordered_columns = sorted(all_columns)
        
        # Preparar ruta: si el usuario solo pasa un nombre de archivo, guardarlo
        # en data/csv/<filename> para mantener el directorio ra√≠z limpio.
        if not os.path.dirname(filename):
            csv_dir = self.DATA_ROOT / "csv"
            csv_dir.mkdir(parents=True, exist_ok=True)
            fullpath = csv_dir / filename
        else:
            fullpath = Path(filename)
            if fullpath.parent:
                fullpath.parent.mkdir(parents=True, exist_ok=True)

        # Escribir archivo CSV
        with open(str(fullpath), 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=ordered_columns)
            writer.writeheader()

            for article in articles:
                # Limpiar cada valor para evitar errores en CSV
                clean_article = {}
                for col in ordered_columns:
                    value = article.get(col, "")
                    # Convertir a string y eliminar caracteres problem√°ticos
                    clean_value = str(value).replace('\n', ' ').replace('\r', ' ')
                    clean_article[col] = clean_value

                writer.writerow(clean_article)

        print(f"Datos guardados en CSV: {fullpath}")
        print(f"Total de registros: {len(articles)}")
        print(f"Columnas incluidas: {len(ordered_columns)}")

    def save_to_json(self, articles: List[Dict], filename: str):
        """
        Guarda los art√≠culos extra√≠dos en un archivo JSON.
        
        M√©todo alternativo de exportaci√≥n que guarda los datos en formato JSON,
        preservando la estructura completa de los datos incluyendo listas y
        objetos anidados. √ötil para procesamiento program√°tico posterior.
        
        Args:
            articles (List[Dict]): Lista de art√≠culos obtenida de scrape_all()
                o extract_articles().
            filename (str): Ruta y nombre del archivo JSON a crear.
                Si no incluye extensi√≥n .json, se recomienda agregarla.
        
        Features:
            - Preserva estructura completa de datos
            - Indentaci√≥n de 2 espacios para legibilidad
            - Encoding UTF-8 con caracteres Unicode
            - Formato JSON est√°ndar compatible con cualquier lenguaje
        
        JSON Structure:
            [
                {
                    "id": "...",
                    "title": "...",
                    "authors": "Author1; Author2",
                    "abstract": "...",
                    ...
                },
                ...
            ]
        
        Advantages over CSV:
            - Preserva tipos de datos (bool, null, etc.)
            - Mejor para datos anidados
            - F√°cil de parsear en cualquier lenguaje
            - No necesita escapar comillas o caracteres especiales
        
        Example:
            >>> articles = scraper.scrape_all("neural networks", max_results=100)
            >>> scraper.save_to_json(articles, "neural_networks.json")
            üíæ Datos guardados en JSON: neural_networks.json
            
            >>> # Leer con Python
            >>> import json
            >>> with open("neural_networks.json", 'r') as f:
            ...     data = json.load(f)
            >>> print(f"Loaded {len(data)} articles")
        
        Reading in Other Languages:
            JavaScript:
                const data = require('./articles.json');
            
            R:
                library(jsonlite)
                data <- fromJSON("articles.json")
            
            Julia:
                using JSON
                data = JSON.parsefile("articles.json")
        
        Note:
            Para datasets muy grandes (>100MB), considerar usar CSV que
            es m√°s eficiente en espacio y puede cargarse parcialmente.
        """
        # Guardar JSON en data/json si no se especifica ruta
        if not os.path.dirname(filename):
            json_dir = self.DATA_ROOT / "json"
            json_dir.mkdir(parents=True, exist_ok=True)
            fullpath = json_dir / filename
        else:
            fullpath = Path(filename)
            if fullpath.parent:
                fullpath.parent.mkdir(parents=True, exist_ok=True)

        with open(str(fullpath), "w", encoding="utf-8") as f:
            json.dump(articles, f, indent=2, ensure_ascii=False)
        print(f"Datos guardados en JSON: {fullpath}")

